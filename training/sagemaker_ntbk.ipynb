{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:34:40.509966Z",
     "start_time": "2025-07-30T12:34:40.326384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# 2) Hard‑code (or read from env var) the execution‑role ARN you created\n",
    "role = \"arn:aws:iam::371087393859:role/defaultrole\"\n",
    "bucket = \"ir-sagemaker\"\n",
    "session = boto3.Session(profile_name=\"lprofile\", region_name=\"us-east-1\")\n",
    "\n",
    "sm_session = sagemaker.Session(boto_session=session, default_bucket=bucket)"
   ],
   "id": "6f0f99a1aa518973",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:34:42.391357Z",
     "start_time": "2025-07-30T12:34:42.255145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sm_session.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sm_session.boto_region_name}\")\n"
   ],
   "id": "2bb382bdca8243f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::371087393859:role/defaultrole\n",
      "sagemaker bucket: ir-sagemaker\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:34:44.342594Z",
     "start_time": "2025-07-30T12:34:44.339440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "print(sys.version)"
   ],
   "id": "f1e67dc7e9146bbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:34:51.691351Z",
     "start_time": "2025-07-30T12:34:51.688216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "bucket = sm_session.default_bucket()\n",
    "prefix = \"modernbert\"\n",
    "\n",
    "train_uri = f\"s3://{bucket}/{prefix}/train/train.jsonl\"\n",
    "val_uri   = f\"s3://{bucket}/{prefix}/val/val.jsonl\"\n",
    "test_uri  = f\"s3://{bucket}/{prefix}/test/test.jsonl\""
   ],
   "id": "da15a52c29c320e1",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T22:13:04.293995Z",
     "start_time": "2025-07-23T22:13:04.278233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_uri = S3Uploader.upload(\"modernbert/data/train/train.jsonl\", f\"s3://{bucket}/{prefix}/train\")\n",
    "val_uri = S3Uploader.upload(\"modernbert/data/val/val.jsonl\",   f\"s3://{bucket}/{prefix}/val\")\n",
    "test_uri = S3Uploader.upload(\"modernbert/data/test/test.jsonl\", f\"s3://{bucket}/{prefix}/test\")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[38;2;255;0;0m╭─\u001B[0m\u001B[38;2;255;0;0m──────────────────────────────\u001B[0m\u001B[38;2;255;0;0m \u001B[0m\u001B[1;38;2;255;0;0mTraceback \u001B[0m\u001B[1;2;38;2;255;0;0m(most recent call last)\u001B[0m\u001B[38;2;255;0;0m \u001B[0m\u001B[38;2;255;0;0m───────────────────────────────\u001B[0m\u001B[38;2;255;0;0m─╮\u001B[0m\n",
       "\u001B[38;2;255;0;0m│\u001B[0m in <module>:2                                                                                    \u001B[38;2;255;0;0m│\u001B[0m\n",
       "\u001B[38;2;255;0;0m│\u001B[0m                                                                                                  \u001B[38;2;255;0;0m│\u001B[0m\n",
       "\u001B[38;2;255;0;0m│\u001B[0m   \u001B[2m1 \u001B[0m\u001B[94mfrom\u001B[0m\u001B[90m \u001B[0m\u001B[4;96msagemaker\u001B[0m\u001B[4;96m.\u001B[0m\u001B[4;96ms3\u001B[0m\u001B[90m \u001B[0m\u001B[94mimport\u001B[0m S3Uploader                                                          \u001B[38;2;255;0;0m│\u001B[0m\n",
       "\u001B[38;2;255;0;0m│\u001B[0m \u001B[31m❱ \u001B[0m2 train_uri = S3Uploader.upload(\u001B[33m\"\u001B[0m\u001B[33mmodernbert/data/train/train.jsonl\u001B[0m\u001B[33m\"\u001B[0m, \u001B[33mf\u001B[0m\u001B[33m\"\u001B[0m\u001B[33ms3://\u001B[0m\u001B[33m{\u001B[0mbucket\u001B[33m}\u001B[0m\u001B[33m/\u001B[0m\u001B[33m{\u001B[0mpref     \u001B[38;2;255;0;0m│\u001B[0m\n",
       "\u001B[38;2;255;0;0m│\u001B[0m   \u001B[2m3 \u001B[0mval_uri = S3Uploader.upload(\u001B[33m\"\u001B[0m\u001B[33mmodernbert/data/val/val.jsonl\u001B[0m\u001B[33m\"\u001B[0m,   \u001B[33mf\u001B[0m\u001B[33m\"\u001B[0m\u001B[33ms3://\u001B[0m\u001B[33m{\u001B[0mbucket\u001B[33m}\u001B[0m\u001B[33m/\u001B[0m\u001B[33m{\u001B[0mprefix\u001B[33m}\u001B[0m\u001B[33m/\u001B[0m     \u001B[38;2;255;0;0m│\u001B[0m\n",
       "\u001B[38;2;255;0;0m│\u001B[0m   \u001B[2m4 \u001B[0mtest_uri = S3Uploader.upload(\u001B[33m\"\u001B[0m\u001B[33mmodernbert/data/test/test.jsonl\u001B[0m\u001B[33m\"\u001B[0m, \u001B[33mf\u001B[0m\u001B[33m\"\u001B[0m\u001B[33ms3://\u001B[0m\u001B[33m{\u001B[0mbucket\u001B[33m}\u001B[0m\u001B[33m/\u001B[0m\u001B[33m{\u001B[0mprefix\u001B[33m}\u001B[0m     \u001B[38;2;255;0;0m│\u001B[0m\n",
       "\u001B[38;2;255;0;0m│\u001B[0m   \u001B[2m5 \u001B[0m                                                                                             \u001B[38;2;255;0;0m│\u001B[0m\n",
       "\u001B[38;2;255;0;0m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n",
       "\u001B[1;91mNameError: \u001B[0mname \u001B[38;2;0;135;0m'prefix'\u001B[0m is not defined\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭─────────────────────────────── </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in &lt;module&gt;:2                                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">sagemaker.s3</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> S3Uploader                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2 train_uri = S3Uploader.upload(<span style=\"color: #808000; text-decoration-color: #808000\">\"modernbert/data/train/train.jsonl\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">f\"s3://{</span>bucket<span style=\"color: #808000; text-decoration-color: #808000\">}/{</span>pref     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>val_uri = S3Uploader.upload(<span style=\"color: #808000; text-decoration-color: #808000\">\"modernbert/data/val/val.jsonl\"</span>,   <span style=\"color: #808000; text-decoration-color: #808000\">f\"s3://{</span>bucket<span style=\"color: #808000; text-decoration-color: #808000\">}/{</span>prefix<span style=\"color: #808000; text-decoration-color: #808000\">}/</span>     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>test_uri = S3Uploader.upload(<span style=\"color: #808000; text-decoration-color: #808000\">\"modernbert/data/test/test.jsonl\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">f\"s3://{</span>bucket<span style=\"color: #808000; text-decoration-color: #808000\">}/{</span>prefix<span style=\"color: #808000; text-decoration-color: #808000\">}</span>     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span>                                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008700; text-decoration-color: #008700\">'prefix'</span> is not defined\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:35:00.741162Z",
     "start_time": "2025-07-30T12:35:00.708304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "metric_definitions=[\n",
    "    {'Name': 'loss', 'Regex': \"'loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'learning_rate', 'Regex': \"'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_loss', 'Regex': \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_accuracy', 'Regex': \"'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_f1', 'Regex': \"'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_precision', 'Regex': \"'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_recall', 'Regex': \"'eval_recall': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_runtime', 'Regex': \"'eval_runtime': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_samples_per_second', 'Regex': \"'eval_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'epoch', 'Regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"}]\n",
    "\n",
    "hyper = {\"learning_rate\":3e-5,\n",
    "         \"num_train_epochs\":5,\n",
    "         \"temperature\":0.05,\n",
    "         \"deepspeed\": \"ds_zero3.json\"}\n",
    "\n",
    "est = HuggingFace(\n",
    "    entry_point=\"train_sm.py\",\n",
    "    source_dir=\"modernbert\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    instance_count=1,\n",
    "    distribution={\"mpi\": {\"enabled\": True}},\n",
    "    transformers_version=\"4.49.0\", pytorch_version=\"2.5.1\", py_version=\"py311\",\n",
    "    hyperparameters=hyper,\n",
    "    metric_definitions=metric_definitions,\n",
    "    environment={\n",
    "        \"PYTORCH_CUDA_ALLOC_CONF\": \"expandable_segments:True\",\n",
    "        \"NCCL_DEBUG\": \"INFO\"\n",
    "    },\n",
    "    output_path=f\"s3://{bucket}/{prefix}/outputs\"\n",
    ")"
   ],
   "id": "f2ea6d1b5bdecbf2",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:17:09.715027Z",
     "start_time": "2025-07-30T12:35:05.208193Z"
    }
   },
   "cell_type": "code",
   "source": "est.fit({\"train\": train_uri, \"val\": val_uri, \"test\": test_uri})",
   "id": "f43f69a1cd6ff444",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2025-07-30-12-35-05-226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-30 12:35:11 Starting - Starting the training job\n",
      "2025-07-30 12:35:11 Pending - Training job waiting for capacity......\n",
      "2025-07-30 12:36:05 Pending - Preparing the instances for training...\n",
      "2025-07-30 12:36:39 Downloading - Downloading input data...\n",
      "2025-07-30 12:36:49 Downloading - Downloading the training image.....................\n",
      "2025-07-30 12:40:41 Training - Training image download completed. Training in progress.....\u001B[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001B[0m\n",
      "\u001B[34mbash: no job control in this shell\u001B[0m\n",
      "\u001B[34mCUDA compat package should be installed for NVIDIA driver smaller than 550.163.01\u001B[0m\n",
      "\u001B[34mCurrent installed NVIDIA driver version is 550.163.01\u001B[0m\n",
      "\u001B[34mSkipping CUDA compat setup as newer NVIDIA driver is installed\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:21,571 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:21,608 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:21,617 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:21,619 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:23,109 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: torch==2.5.1 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.5.1+cu124)\u001B[0m\n",
      "\u001B[34mCollecting deepspeed==0.17.1 (from -r requirements.txt (line 2))\u001B[0m\n",
      "\u001B[34mDownloading deepspeed-0.17.1.tar.gz (1.5 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 39.2 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mPreparing metadata (setup.py): started\u001B[0m\n",
      "\u001B[34mPreparing metadata (setup.py): finished with status 'done'\u001B[0m\n",
      "\u001B[34mCollecting transformers==4.54.0 (from -r requirements.txt (line 3))\u001B[0m\n",
      "\u001B[34mDownloading transformers-4.54.0-py3-none-any.whl.metadata (41 kB)\u001B[0m\n",
      "\u001B[34mCollecting flash-attn==2.7.4.post1 (from -r requirements.txt (line 6))\u001B[0m\n",
      "\u001B[34mDownloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 89.7 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mPreparing metadata (setup.py): started\u001B[0m\n",
      "\u001B[34mPreparing metadata (setup.py): finished with status 'done'\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: triton>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (3.1.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (1.11.1.3)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (24.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (3.18.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (4.14.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (3.5)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (3.1.6)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (2024.12.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (1.13.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: einops in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.17.1->-r requirements.txt (line 2)) (0.8.1)\u001B[0m\n",
      "\u001B[34mCollecting hjson (from deepspeed==0.17.1->-r requirements.txt (line 2))\u001B[0m\n",
      "\u001B[34mDownloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: msgpack in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.17.1->-r requirements.txt (line 2)) (1.1.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.17.1->-r requirements.txt (line 2)) (1.26.4)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.17.1->-r requirements.txt (line 2)) (7.0.0)\u001B[0m\n",
      "\u001B[34mCollecting py-cpuinfo (from deepspeed==0.17.1->-r requirements.txt (line 2))\u001B[0m\n",
      "\u001B[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pydantic>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.17.1->-r requirements.txt (line 2)) (2.11.7)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.17.1->-r requirements.txt (line 2)) (4.66.5)\u001B[0m\n",
      "\u001B[34mCollecting nvidia-ml-py (from deepspeed==0.17.1->-r requirements.txt (line 2))\u001B[0m\n",
      "\u001B[34mDownloading nvidia_ml_py-12.575.51-py3-none-any.whl.metadata (9.3 kB)\u001B[0m\n",
      "\u001B[34mCollecting huggingface-hub<1.0,>=0.34.0 (from transformers==4.54.0->-r requirements.txt (line 3))\u001B[0m\n",
      "\u001B[34mDownloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers==4.54.0->-r requirements.txt (line 3)) (6.0.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers==4.54.0->-r requirements.txt (line 3)) (2024.11.6)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers==4.54.0->-r requirements.txt (line 3)) (2.32.4)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers==4.54.0->-r requirements.txt (line 3)) (0.21.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from transformers==4.54.0->-r requirements.txt (line 3)) (0.5.3)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.5.1->-r requirements.txt (line 1)) (1.3.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.54.0->-r requirements.txt (line 3)) (1.1.5)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0.0->deepspeed==0.17.1->-r requirements.txt (line 2)) (0.7.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0.0->deepspeed==0.17.1->-r requirements.txt (line 2)) (2.33.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0.0->deepspeed==0.17.1->-r requirements.txt (line 2)) (0.4.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.5.1->-r requirements.txt (line 1)) (3.0.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.54.0->-r requirements.txt (line 3)) (3.4.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.54.0->-r requirements.txt (line 3)) (3.10)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.54.0->-r requirements.txt (line 3)) (2.5.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.54.0->-r requirements.txt (line 3)) (2025.7.14)\u001B[0m\n",
      "\u001B[34mDownloading transformers-4.54.0-py3-none-any.whl (11.2 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.2/11.2 MB 159.1 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 558.8/558.8 kB 59.5 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\u001B[0m\n",
      "\u001B[34mDownloading nvidia_ml_py-12.575.51-py3-none-any.whl (47 kB)\u001B[0m\n",
      "\u001B[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001B[0m\n",
      "\u001B[34mBuilding wheels for collected packages: deepspeed, flash-attn\u001B[0m\n",
      "\u001B[34mDEPRECATION: Building 'deepspeed' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'deepspeed'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001B[0m\n",
      "\u001B[34mBuilding wheel for deepspeed (setup.py): started\u001B[0m\n",
      "\u001B[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001B[0m\n",
      "\u001B[34mCreated wheel for deepspeed: filename=deepspeed-0.17.1-py3-none-any.whl size=1690871 sha256=715f8e876da25a28f63959ddc88f781ddff155252a6d2bdada173c0d3dfdeb4a\u001B[0m\n",
      "\u001B[34mStored in directory: /root/.cache/pip/wheels/34/86/36/22db26525829160fd1c4add33d8a834ec046b90abf45cd363b\u001B[0m\n",
      "\u001B[34mDEPRECATION: Building 'flash-attn' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'flash-attn'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001B[0m\n",
      "\u001B[34mBuilding wheel for flash-attn (setup.py): started\u001B[0m\n",
      "\u001B[34mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001B[0m\n",
      "\u001B[34mCreated wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187815463 sha256=d944fc7d2f962bce83fc4708c2fc0c21eaf8255962a0b350ae919362a51b7ef2\u001B[0m\n",
      "\u001B[34mStored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\u001B[0m\n",
      "\u001B[34mSuccessfully built deepspeed flash-attn\u001B[0m\n",
      "\u001B[34mInstalling collected packages: py-cpuinfo, nvidia-ml-py, hjson, huggingface-hub, flash-attn, deepspeed, transformers\u001B[0m\n",
      "\u001B[34mAttempting uninstall: huggingface-hub\u001B[0m\n",
      "\u001B[34mFound existing installation: huggingface-hub 0.29.1\u001B[0m\n",
      "\u001B[34mUninstalling huggingface-hub-0.29.1:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled huggingface-hub-0.29.1\u001B[0m\n",
      "\u001B[34mAttempting uninstall: flash-attn\u001B[0m\n",
      "\u001B[34mFound existing installation: flash-attn 2.7.3\u001B[0m\n",
      "\u001B[34mUninstalling flash-attn-2.7.3:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled flash-attn-2.7.3\u001B[0m\n",
      "\u001B[34mAttempting uninstall: transformers\u001B[0m\n",
      "\u001B[34mFound existing installation: transformers 4.49.0\u001B[0m\n",
      "\u001B[34mUninstalling transformers-4.49.0:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled transformers-4.49.0\u001B[0m\n",
      "\u001B[34mSuccessfully installed deepspeed-0.17.1 flash-attn-2.7.4.post1 hjson-3.1.0 huggingface-hub-0.34.3 nvidia-ml-py-12.575.51 py-cpuinfo-9.0.0 transformers-4.54.0\u001B[0m\n",
      "\u001B[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:50,459 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:50,459 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:50,519 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:50,566 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:50,575 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:50,575 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:50,576 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:50,576 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1:4'] process_per_hosts: 4 num_processes: 4\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:50,576 sagemaker-training-toolkit INFO     Network interface name: eth0\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:50,614 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:50,662 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:50,673 sagemaker-training-toolkit INFO     Invoking user script\u001B[0m\n",
      "\u001B[34mTraining Env:\u001B[0m\n",
      "\u001B[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"deepspeed\": \"ds_zero3.json\",\n",
      "        \"learning_rate\": 3e-05,\n",
      "        \"num_train_epochs\": 5,\n",
      "        \"temperature\": 0.05\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2025-07-30-12-35-05-226\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://ir-sagemaker/huggingface-pytorch-training-2025-07-30-12-35-05-226/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_sm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"train_sm.py\"\u001B[0m\n",
      "\u001B[34m}\u001B[0m\n",
      "\u001B[34mEnvironment variables:\u001B[0m\n",
      "\u001B[34mSM_HOSTS=[\"algo-1\"]\u001B[0m\n",
      "\u001B[34mSM_NETWORK_INTERFACE_NAME=eth0\u001B[0m\n",
      "\u001B[34mSM_HPS={\"deepspeed\":\"ds_zero3.json\",\"learning_rate\":3e-05,\"num_train_epochs\":5,\"temperature\":0.05}\u001B[0m\n",
      "\u001B[34mSM_USER_ENTRY_POINT=train_sm.py\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true}\u001B[0m\n",
      "\u001B[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\u001B[0m\n",
      "\u001B[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001B[0m\n",
      "\u001B[34mSM_CHANNELS=[\"test\",\"train\",\"val\"]\u001B[0m\n",
      "\u001B[34mSM_CURRENT_HOST=algo-1\u001B[0m\n",
      "\u001B[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001B[0m\n",
      "\u001B[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001B[0m\n",
      "\u001B[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001B[0m\n",
      "\u001B[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001B[0m\n",
      "\u001B[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001B[0m\n",
      "\u001B[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001B[0m\n",
      "\u001B[34mSM_IS_HETERO=false\u001B[0m\n",
      "\u001B[34mSM_MODULE_NAME=train_sm\u001B[0m\n",
      "\u001B[34mSM_LOG_LEVEL=20\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001B[0m\n",
      "\u001B[34mSM_INPUT_DIR=/opt/ml/input\u001B[0m\n",
      "\u001B[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DIR=/opt/ml/output\u001B[0m\n",
      "\u001B[34mSM_NUM_CPUS=48\u001B[0m\n",
      "\u001B[34mSM_NUM_GPUS=4\u001B[0m\n",
      "\u001B[34mSM_NUM_NEURONS=0\u001B[0m\n",
      "\u001B[34mSM_MODEL_DIR=/opt/ml/model\u001B[0m\n",
      "\u001B[34mSM_MODULE_DIR=s3://ir-sagemaker/huggingface-pytorch-training-2025-07-30-12-35-05-226/source/sourcedir.tar.gz\u001B[0m\n",
      "\u001B[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"deepspeed\":\"ds_zero3.json\",\"learning_rate\":3e-05,\"num_train_epochs\":5,\"temperature\":0.05},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2025-07-30-12-35-05-226\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://ir-sagemaker/huggingface-pytorch-training-2025-07-30-12-35-05-226/source/sourcedir.tar.gz\",\"module_name\":\"train_sm\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"train_sm.py\"}\u001B[0m\n",
      "\u001B[34mSM_USER_ARGS=[\"--deepspeed\",\"ds_zero3.json\",\"--learning_rate\",\"3e-05\",\"--num_train_epochs\",\"5\",\"--temperature\",\"0.05\"]\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001B[0m\n",
      "\u001B[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001B[0m\n",
      "\u001B[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001B[0m\n",
      "\u001B[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001B[0m\n",
      "\u001B[34mSM_HP_DEEPSPEED=ds_zero3.json\u001B[0m\n",
      "\u001B[34mSM_HP_LEARNING_RATE=3e-05\u001B[0m\n",
      "\u001B[34mSM_HP_NUM_TRAIN_EPOCHS=5\u001B[0m\n",
      "\u001B[34mSM_HP_TEMPERATURE=0.05\u001B[0m\n",
      "\u001B[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python311.zip:/opt/conda/lib/python3.11:/opt/conda/lib/python3.11/lib-dynload:/opt/conda/lib/python3.11/site-packages\u001B[0m\n",
      "\u001B[34mInvoking script with the following command:\u001B[0m\n",
      "\u001B[34mmpirun --host algo-1:4 -np 4 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.11/site-packages/gethostname.cpython-311-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_CURRENT_INSTANCE_TYPE -x SM_CURRENT_INSTANCE_GROUP -x SM_CURRENT_INSTANCE_GROUP_HOSTS -x SM_INSTANCE_GROUPS -x SM_INSTANCE_GROUPS_DICT -x SM_DISTRIBUTION_INSTANCE_GROUPS -x SM_IS_HETERO -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_NUM_NEURONS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TEST -x SM_CHANNEL_TRAIN -x SM_CHANNEL_VAL -x SM_HP_DEEPSPEED -x SM_HP_LEARNING_RATE -x SM_HP_NUM_TRAIN_EPOCHS -x SM_HP_TEMPERATURE -x PYTHONPATH /opt/conda/bin/python3.11 -m mpi4py train_sm.py --deepspeed ds_zero3.json --learning_rate 3e-05 --num_train_epochs 5 --temperature 0.05\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:50,710 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:50,721 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:50,721 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001B[0m\n",
      "\u001B[34m2025-07-30 12:41:50,722 sagemaker-training-toolkit INFO     smdistributed.dataparallel not found or using an older version without custom exceptions.SM training toolkit will track user script error only\u001B[0m\n",
      "\u001B[34mData for JOB [41262,1] offset 0 Total slots allocated 4\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: algo-1#011Num slots: 4#011Max slots: 0#011Num procs: 4\n",
      " #011Process OMPI jobid: [41262,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41262,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [41262,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41262,1] App: 0 Process rank: 3 Bound: N/A\n",
      " =============================================================\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:[2025-07-30 12:41:54,140] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:[2025-07-30 12:41:54,143] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:[2025-07-30 12:41:54,147] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:41:54,148] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:df: /root/.triton/autotune: No such file or directory\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:[2025-07-30 12:41:56,208] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:[2025-07-30 12:41:56,213] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:41:56,216] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:[2025-07-30 12:41:56,231] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading builder script: 0.00B [00:00, ?B/s]\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading builder script: 4.20kB [00:00, 16.4MB/s]\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading builder script: 0.00B [00:00, ?B/s]\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading builder script: 4.20kB [00:00, 18.3MB/s]\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading builder script: 0.00B [00:00, ?B/s]\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading builder script: 4.20kB [00:00, 18.6MB/s]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading builder script: 0.00B [00:00, ?B/s]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading builder script: 7.56kB [00:00, 26.8MB/s]\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading builder script: 0.00B [00:00, ?B/s]\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading builder script: 7.56kB [00:00, 28.3MB/s]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading builder script: 0.00B [00:00, ?B/s]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading builder script: 7.38kB [00:00, 24.2MB/s]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading builder script: 0.00B [00:00, ?B/s]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading builder script: 6.79kB [00:00, 30.1MB/s]\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading builder script: 0.00B [00:00, ?B/s]\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading builder script: 6.79kB [00:00, 26.6MB/s]\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:loading file tokenizer.model from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:loading file added_tokens.json from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/special_tokens_map.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer_config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:loading file tokenizer.model from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:loading file added_tokens.json from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/special_tokens_map.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer_config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:loading file chat_template.jinja from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:loading file chat_template.jinja from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:loading file tokenizer.model from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:loading file tokenizer.model from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:loading file added_tokens.json from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/special_tokens_map.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:loading file added_tokens.json from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/special_tokens_map.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer_config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:loading file chat_template.jinja from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer_config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:loading file chat_template.jinja from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:loading file tokenizer.model from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:loading file tokenizer.model from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:loading file added_tokens.json from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/special_tokens_map.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer_config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:loading file chat_template.jinja from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:loading file added_tokens.json from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/special_tokens_map.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer_config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:loading file chat_template.jinja from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:loading file tokenizer.model from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:loading file tokenizer.model from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:loading file added_tokens.json from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/special_tokens_map.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer_config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:loading file chat_template.jinja from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:loading file added_tokens.json from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/special_tokens_map.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer_config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:loading file chat_template.jinja from cache at None\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Model config ModernBertConfig {\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"architectures\": [\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:    \"ModernBertForMaskedLM\"\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  ],\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"attention_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"attention_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"bos_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"classifier_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"classifier_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"classifier_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"classifier_pooling\": \"mean\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"cls_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"decoder_bias\": true,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"deterministic_flash_attn\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"embedding_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"eos_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"global_attn_every_n_layers\": 3,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"global_rope_theta\": 160000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"gradient_checkpointing\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"hidden_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"hidden_size\": 768,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"initializer_cutoff_factor\": 2.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"initializer_range\": 0.02,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"intermediate_size\": 1152,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"layer_norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"local_attention\": 128,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"local_rope_theta\": 10000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"max_position_embeddings\": 8192,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"mlp_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"mlp_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"model_type\": \"modernbert\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"norm_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"num_attention_heads\": 12,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"num_hidden_layers\": 22,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"pad_token_id\": 50283,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"position_embedding_type\": \"absolute\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"repad_logits_with_grad\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"sep_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"sparse_pred_ignore_index\": -100,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"sparse_prediction\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"torch_dtype\": \"float32\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"transformers_version\": \"4.54.0\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"vocab_size\": 50368\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:Model config ModernBertConfig {\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"architectures\": [\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:    \"ModernBertForMaskedLM\"\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  ],\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"attention_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"attention_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"bos_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"classifier_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"classifier_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"classifier_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"classifier_pooling\": \"mean\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"cls_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"decoder_bias\": true,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"deterministic_flash_attn\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"embedding_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"eos_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"global_attn_every_n_layers\": 3,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"global_rope_theta\": 160000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"gradient_checkpointing\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"hidden_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"hidden_size\": 768,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"initializer_cutoff_factor\": 2.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"initializer_range\": 0.02,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"intermediate_size\": 1152,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"layer_norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"local_attention\": 128,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"local_rope_theta\": 10000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"max_position_embeddings\": 8192,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"mlp_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"mlp_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"model_type\": \"modernbert\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"norm_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"num_attention_heads\": 12,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"num_hidden_layers\": 22,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"pad_token_id\": 50283,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"position_embedding_type\": \"absolute\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"repad_logits_with_grad\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"sep_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"sparse_pred_ignore_index\": -100,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"sparse_prediction\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"torch_dtype\": \"float32\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"transformers_version\": \"4.54.0\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"vocab_size\": 50368\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:Model config ModernBertConfig {\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"architectures\": [\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:    \"ModernBertForMaskedLM\"\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  ],\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"attention_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"attention_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"bos_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"classifier_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"classifier_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"classifier_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"classifier_pooling\": \"mean\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"cls_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"decoder_bias\": true,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"deterministic_flash_attn\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"embedding_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"eos_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"global_attn_every_n_layers\": 3,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"global_rope_theta\": 160000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"gradient_checkpointing\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"hidden_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"hidden_size\": 768,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"initializer_cutoff_factor\": 2.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"initializer_range\": 0.02,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"intermediate_size\": 1152,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"layer_norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"local_attention\": 128,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"local_rope_theta\": 10000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"max_position_embeddings\": 8192,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"mlp_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"mlp_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"model_type\": \"modernbert\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"norm_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"num_attention_heads\": 12,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"num_hidden_layers\": 22,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"pad_token_id\": 50283,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"position_embedding_type\": \"absolute\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"repad_logits_with_grad\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"sep_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"sparse_pred_ignore_index\": -100,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"sparse_prediction\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"torch_dtype\": \"float32\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"transformers_version\": \"4.54.0\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"vocab_size\": 50368\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Model config ModernBertConfig {\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"architectures\": [\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:    \"ModernBertForMaskedLM\"\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  ],\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"attention_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"attention_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"bos_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"classifier_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"classifier_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"classifier_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"classifier_pooling\": \"mean\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"cls_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"decoder_bias\": true,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"deterministic_flash_attn\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"embedding_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"eos_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"global_attn_every_n_layers\": 3,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"global_rope_theta\": 160000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"gradient_checkpointing\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"hidden_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"hidden_size\": 768,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"initializer_cutoff_factor\": 2.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"initializer_range\": 0.02,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"intermediate_size\": 1152,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"layer_norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"local_attention\": 128,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"local_rope_theta\": 10000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"max_position_embeddings\": 8192,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"mlp_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"mlp_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"model_type\": \"modernbert\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"norm_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"num_attention_heads\": 12,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"num_hidden_layers\": 22,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"pad_token_id\": 50283,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"position_embedding_type\": \"absolute\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"repad_logits_with_grad\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"sep_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"sparse_pred_ignore_index\": -100,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"sparse_prediction\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"torch_dtype\": \"float32\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"transformers_version\": \"4.54.0\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"vocab_size\": 50368\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:Model config ModernBertConfig {\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"architectures\": [\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:    \"ModernBertForMaskedLM\"\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  ],\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"attention_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"attention_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"bos_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"classifier_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"classifier_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"classifier_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"classifier_pooling\": \"mean\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"cls_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"decoder_bias\": true,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"deterministic_flash_attn\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"embedding_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"eos_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"global_attn_every_n_layers\": 3,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"global_rope_theta\": 160000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"gradient_checkpointing\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"hidden_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"hidden_size\": 768,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"initializer_cutoff_factor\": 2.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"initializer_range\": 0.02,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"intermediate_size\": 1152,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"layer_norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"local_attention\": 128,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"local_rope_theta\": 10000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"max_position_embeddings\": 8192,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"mlp_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"mlp_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"model_type\": \"modernbert\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"norm_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"num_attention_heads\": 12,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"num_hidden_layers\": 22,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"pad_token_id\": 50283,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"position_embedding_type\": \"absolute\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"repad_logits_with_grad\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"sep_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"sparse_pred_ignore_index\": -100,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"sparse_prediction\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"torch_dtype\": \"float32\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"transformers_version\": \"4.54.0\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"vocab_size\": 50368\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Model config ModernBertConfig {\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"architectures\": [\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:    \"ModernBertForMaskedLM\"\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  ],\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"attention_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"attention_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"bos_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"classifier_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"classifier_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"classifier_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"classifier_pooling\": \"mean\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"cls_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"decoder_bias\": true,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"deterministic_flash_attn\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"embedding_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"eos_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"global_attn_every_n_layers\": 3,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"global_rope_theta\": 160000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"gradient_checkpointing\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"hidden_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"hidden_size\": 768,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"initializer_cutoff_factor\": 2.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"initializer_range\": 0.02,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"intermediate_size\": 1152,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"layer_norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"local_attention\": 128,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"local_rope_theta\": 10000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"max_position_embeddings\": 8192,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"mlp_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"mlp_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"model_type\": \"modernbert\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"norm_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"num_attention_heads\": 12,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"num_hidden_layers\": 22,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"pad_token_id\": 50283,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"position_embedding_type\": \"absolute\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"repad_logits_with_grad\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"sep_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"sparse_pred_ignore_index\": -100,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"sparse_prediction\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"torch_dtype\": \"float32\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"transformers_version\": \"4.54.0\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"vocab_size\": 50368\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Model config ModernBertConfig {\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"architectures\": [\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:    \"ModernBertForMaskedLM\"\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  ],\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"attention_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"attention_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"classifier_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"classifier_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"classifier_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"classifier_pooling\": \"mean\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"cls_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"decoder_bias\": true,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"deterministic_flash_attn\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"embedding_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"global_attn_every_n_layers\": 3,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"global_rope_theta\": 160000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"gradient_checkpointing\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_size\": 768,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"initializer_cutoff_factor\": 2.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"initializer_range\": 0.02,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"intermediate_size\": 1152,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"layer_norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"local_attention\": 128,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"local_rope_theta\": 10000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"max_position_embeddings\": 8192,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"mlp_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"mlp_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"model_type\": \"modernbert\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"norm_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"num_attention_heads\": 12,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"num_hidden_layers\": 22,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 50283,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"position_embedding_type\": \"absolute\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"repad_logits_with_grad\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"sep_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"sparse_pred_ignore_index\": -100,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"sparse_prediction\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"torch_dtype\": \"float32\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.54.0\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"vocab_size\": 50368\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Model config ModernBertConfig {\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"architectures\": [\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"ModernBertForMaskedLM\"\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  ],\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"attention_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"attention_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"classifier_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"classifier_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"classifier_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"classifier_pooling\": \"mean\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"cls_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"decoder_bias\": true,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"deterministic_flash_attn\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"embedding_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"global_attn_every_n_layers\": 3,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"global_rope_theta\": 160000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"gradient_checkpointing\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_size\": 768,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"initializer_cutoff_factor\": 2.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"initializer_range\": 0.02,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"intermediate_size\": 1152,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"layer_norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"local_attention\": 128,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"local_rope_theta\": 10000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"max_position_embeddings\": 8192,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"mlp_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"mlp_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"model_type\": \"modernbert\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"norm_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"num_attention_heads\": 12,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"num_hidden_layers\": 22,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 50283,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"position_embedding_type\": \"absolute\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"repad_logits_with_grad\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"sep_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"sparse_pred_ignore_index\": -100,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"sparse_prediction\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"torch_dtype\": \"float32\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.54.0\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"vocab_size\": 50368\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:Detected flash_attn version: 2.7.4.post1\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Detected flash_attn version: 2.7.4.post1\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:Detected flash_attn version: 2.7.4.post1\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Detected flash_attn version: 2.7.4.post1\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:Detected flash_attn version: 2.7.4.post1\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Detected flash_attn version: 2.7.4.post1\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Detected flash_attn version: 2.7.4.post1\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Detected flash_attn version: 2.7.4.post1\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:All model checkpoint weights were used when initializing ModernBertForMaskedLM.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:All model checkpoint weights were used when initializing ModernBertForMaskedLM.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:All model checkpoint weights were used when initializing ModernBertForMaskedLM.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:All model checkpoint weights were used when initializing ModernBertForMaskedLM.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:All model checkpoint weights were used when initializing ModernBertForMaskedLM.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:All model checkpoint weights were used when initializing ModernBertForMaskedLM.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Model config ModernBertConfig {\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"architectures\": [\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:    \"ModernBertForMaskedLM\"\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  ],\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"attention_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"attention_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"bos_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"classifier_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"classifier_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"classifier_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"classifier_pooling\": \"mean\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"cls_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"decoder_bias\": true,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"deterministic_flash_attn\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"embedding_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"eos_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"global_attn_every_n_layers\": 3,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"global_rope_theta\": 160000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"gradient_checkpointing\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"hidden_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"hidden_size\": 768,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"initializer_cutoff_factor\": 2.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"initializer_range\": 0.02,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"intermediate_size\": 1152,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"layer_norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"local_attention\": 128,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"local_rope_theta\": 10000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"max_position_embeddings\": 8192,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"mlp_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"mlp_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"model_type\": \"modernbert\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"norm_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"num_attention_heads\": 12,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"num_hidden_layers\": 22,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"pad_token_id\": 50283,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"position_embedding_type\": \"absolute\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"repad_logits_with_grad\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"sep_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"sparse_pred_ignore_index\": -100,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"sparse_prediction\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"torch_dtype\": \"float32\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"transformers_version\": \"4.54.0\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:  \"vocab_size\": 50368\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:Model config ModernBertConfig {\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"architectures\": [\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:    \"ModernBertForMaskedLM\"\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  ],\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"attention_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"attention_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"bos_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"classifier_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"classifier_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"classifier_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"classifier_pooling\": \"mean\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"cls_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"decoder_bias\": true,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"deterministic_flash_attn\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"embedding_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"eos_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"global_attn_every_n_layers\": 3,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"global_rope_theta\": 160000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"gradient_checkpointing\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"hidden_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"hidden_size\": 768,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"initializer_cutoff_factor\": 2.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"initializer_range\": 0.02,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"intermediate_size\": 1152,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"layer_norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"local_attention\": 128,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"local_rope_theta\": 10000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"max_position_embeddings\": 8192,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"mlp_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"mlp_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"model_type\": \"modernbert\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"norm_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"num_attention_heads\": 12,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"num_hidden_layers\": 22,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"pad_token_id\": 50283,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"position_embedding_type\": \"absolute\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"repad_logits_with_grad\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"sep_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"sparse_pred_ignore_index\": -100,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"sparse_prediction\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"torch_dtype\": \"float32\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"transformers_version\": \"4.54.0\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  \"vocab_size\": 50368\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:Model config ModernBertConfig {\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"architectures\": [\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:    \"ModernBertForMaskedLM\"\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  ],\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"attention_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"attention_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"bos_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"classifier_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"classifier_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"classifier_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"classifier_pooling\": \"mean\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"cls_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"decoder_bias\": true,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"deterministic_flash_attn\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"embedding_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"eos_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"global_attn_every_n_layers\": 3,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"global_rope_theta\": 160000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"gradient_checkpointing\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"hidden_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"hidden_size\": 768,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"initializer_cutoff_factor\": 2.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"initializer_range\": 0.02,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"intermediate_size\": 1152,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"layer_norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"local_attention\": 128,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"local_rope_theta\": 10000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"max_position_embeddings\": 8192,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"mlp_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"mlp_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"model_type\": \"modernbert\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"norm_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"num_attention_heads\": 12,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"num_hidden_layers\": 22,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"pad_token_id\": 50283,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"position_embedding_type\": \"absolute\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"repad_logits_with_grad\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"sep_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"sparse_pred_ignore_index\": -100,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"sparse_prediction\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"torch_dtype\": \"float32\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"transformers_version\": \"4.54.0\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  \"vocab_size\": 50368\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Model config ModernBertConfig {\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"architectures\": [\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:    \"ModernBertForMaskedLM\"\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  ],\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"attention_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"attention_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"bos_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"classifier_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"classifier_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"classifier_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"classifier_pooling\": \"mean\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"cls_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"decoder_bias\": true,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"deterministic_flash_attn\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"embedding_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"eos_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"global_attn_every_n_layers\": 3,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"global_rope_theta\": 160000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"gradient_checkpointing\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"hidden_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"hidden_size\": 768,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"initializer_cutoff_factor\": 2.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"initializer_range\": 0.02,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"intermediate_size\": 1152,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"layer_norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"local_attention\": 128,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"local_rope_theta\": 10000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"max_position_embeddings\": 8192,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"mlp_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"mlp_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"model_type\": \"modernbert\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"norm_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"num_attention_heads\": 12,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"num_hidden_layers\": 22,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"pad_token_id\": 50283,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"position_embedding_type\": \"absolute\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"repad_logits_with_grad\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"sep_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"sparse_pred_ignore_index\": -100,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"sparse_prediction\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"torch_dtype\": \"float32\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"transformers_version\": \"4.54.0\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:  \"vocab_size\": 50368\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:All model checkpoint weights were used when initializing ModernBertForMaskedLM.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:All model checkpoint weights were used when initializing ModernBertForMaskedLM.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Model config ModernBertConfig {\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"architectures\": [\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:    \"ModernBertForMaskedLM\"\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  ],\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"attention_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"attention_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"classifier_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"classifier_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"classifier_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"classifier_pooling\": \"mean\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"cls_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"decoder_bias\": true,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"deterministic_flash_attn\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"embedding_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"global_attn_every_n_layers\": 3,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"global_rope_theta\": 160000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"gradient_checkpointing\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_size\": 768,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"initializer_cutoff_factor\": 2.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"initializer_range\": 0.02,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"intermediate_size\": 1152,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"layer_norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"local_attention\": 128,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"local_rope_theta\": 10000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"max_position_embeddings\": 8192,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"mlp_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"mlp_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"model_type\": \"modernbert\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"norm_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"num_attention_heads\": 12,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"num_hidden_layers\": 22,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 50283,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"position_embedding_type\": \"absolute\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"repad_logits_with_grad\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"sep_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"sparse_pred_ignore_index\": -100,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"sparse_prediction\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"torch_dtype\": \"float32\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.54.0\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  \"vocab_size\": 50368\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Model config ModernBertConfig {\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"architectures\": [\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"ModernBertForMaskedLM\"\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  ],\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"attention_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"attention_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"classifier_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"classifier_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"classifier_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"classifier_pooling\": \"mean\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"cls_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"decoder_bias\": true,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"deterministic_flash_attn\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"embedding_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"global_attn_every_n_layers\": 3,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"global_rope_theta\": 160000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"gradient_checkpointing\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_size\": 768,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"initializer_cutoff_factor\": 2.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"initializer_range\": 0.02,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"intermediate_size\": 1152,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"layer_norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"local_attention\": 128,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"local_rope_theta\": 10000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"max_position_embeddings\": 8192,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"mlp_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"mlp_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"model_type\": \"modernbert\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"norm_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"num_attention_heads\": 12,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"num_hidden_layers\": 22,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 50283,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"position_embedding_type\": \"absolute\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"repad_logits_with_grad\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"sep_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"sparse_pred_ignore_index\": -100,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"sparse_prediction\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"torch_dtype\": \"float32\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.54.0\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  \"vocab_size\": 50368\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:Model config ModernBertConfig {\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"architectures\": [\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:    \"ModernBertForMaskedLM\"\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  ],\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"attention_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"attention_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"bos_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"classifier_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"classifier_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"classifier_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"classifier_pooling\": \"mean\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"cls_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"decoder_bias\": true,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"deterministic_flash_attn\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"embedding_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"eos_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"global_attn_every_n_layers\": 3,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"global_rope_theta\": 160000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"gradient_checkpointing\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"hidden_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"hidden_size\": 768,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"initializer_cutoff_factor\": 2.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"initializer_range\": 0.02,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"intermediate_size\": 1152,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"layer_norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"local_attention\": 128,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"local_rope_theta\": 10000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"max_position_embeddings\": 8192,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"mlp_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"mlp_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"model_type\": \"modernbert\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"norm_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"num_attention_heads\": 12,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"num_hidden_layers\": 22,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"pad_token_id\": 50283,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"position_embedding_type\": \"absolute\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"repad_logits_with_grad\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"sep_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"sparse_pred_ignore_index\": -100,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"sparse_prediction\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"torch_dtype\": \"float32\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"transformers_version\": \"4.54.0\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  \"vocab_size\": 50368\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Model config ModernBertConfig {\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"architectures\": [\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:    \"ModernBertForMaskedLM\"\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  ],\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"attention_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"attention_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"bos_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"classifier_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"classifier_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"classifier_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"classifier_pooling\": \"mean\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"cls_token_id\": 50281,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"decoder_bias\": true,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"deterministic_flash_attn\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"embedding_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"eos_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"global_attn_every_n_layers\": 3,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"global_rope_theta\": 160000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"gradient_checkpointing\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"hidden_activation\": \"gelu\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"hidden_size\": 768,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"initializer_cutoff_factor\": 2.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"initializer_range\": 0.02,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"intermediate_size\": 1152,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"layer_norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"local_attention\": 128,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"local_rope_theta\": 10000.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"max_position_embeddings\": 8192,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"mlp_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"mlp_dropout\": 0.0,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"model_type\": \"modernbert\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"norm_bias\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"norm_eps\": 1e-05,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"num_attention_heads\": 12,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"num_hidden_layers\": 22,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"pad_token_id\": 50283,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"position_embedding_type\": \"absolute\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"repad_logits_with_grad\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"sep_token_id\": 50282,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"sparse_pred_ignore_index\": -100,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"sparse_prediction\": false,\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"torch_dtype\": \"float32\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"transformers_version\": \"4.54.0\",\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:  \"vocab_size\": 50368\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:All model checkpoint weights were used when initializing ModernBertForMaskedLM.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:All model checkpoint weights were used when initializing ModernBertForMaskedLM.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:All model checkpoint weights were used when initializing ModernBertForMaskedLM.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:All model checkpoint weights were used when initializing ModernBertForMaskedLM.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:All model checkpoint weights were used when initializing ModernBertForMaskedLM.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:All model checkpoint weights were used when initializing ModernBertForMaskedLM.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:All model checkpoint weights were used when initializing ModernBertForMaskedLM.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:All model checkpoint weights were used when initializing ModernBertForMaskedLM.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:PyTorch: setting up devices\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:PyTorch: setting up devices\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/accelerate/state.py:247: UserWarning: OMP_NUM_THREADS/MKL_NUM_THREADS unset, we set it at 6 to improve oob performance.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:PyTorch: setting up devices\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:PyTorch: setting up devices\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:PyTorch: setting up devices\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:PyTorch: setting up devices\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:PyTorch: setting up devices\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:PyTorch: setting up devices\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/accelerate/state.py:247: UserWarning: OMP_NUM_THREADS/MKL_NUM_THREADS unset, we set it at 6 to improve oob performance.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/accelerate/state.py:247: UserWarning: OMP_NUM_THREADS/MKL_NUM_THREADS unset, we set it at 6 to improve oob performance.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/accelerate/state.py:247: UserWarning: OMP_NUM_THREADS/MKL_NUM_THREADS unset, we set it at 6 to improve oob performance.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Using auto half precision backend\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Using auto half precision backend\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Currently training with a batch size of: 8\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Currently training with a batch size of: 8\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:00,612] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.1, git-hash=unknown, git-branch=unknown\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:00,612] [INFO] [comm.py:675:init_distributed] cdb=None\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:00,612] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:00,923] [INFO] [engine.py:1325:_configure_distributed_model] ********** distributed groups summary **********\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:#011 self.dp_world_size=4\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:#011 self.mp_world_size=1\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:#011 self.seq_dp_world_size=4\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:#011 self.sequence_parallel_size=1\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:***********************************************\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:01,467] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:01,468] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:01,468] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:01,474] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:01,474] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:01,474] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:01,474] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:01,645] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:01,646] [INFO] [utils.py:782:see_memory_usage] MA 0.56 GB         Max_MA 1.19 GB         CA 1.19 GB         Max_CA 1 GB\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:01,646] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 20.16 GB, percent = 10.8%\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:01,648] [INFO] [stage3.py:170:__init__] Reduce bucket size 500000000\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:01,648] [INFO] [stage3.py:171:__init__] Prefetch bucket size 50000000\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:01,820] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:01,821] [INFO] [utils.py:782:see_memory_usage] MA 0.56 GB         Max_MA 0.56 GB         CA 1.19 GB         Max_CA 1 GB\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:01,821] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 20.15 GB, percent = 10.8%\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:01,825] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Parameter Offload - Persistent parameters statistics: param_count = 184, numel = 66231680\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:02,531] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:02,532] [INFO] [utils.py:782:see_memory_usage] MA 0.14 GB         Max_MA 0.58 GB         CA 1.3 GB         Max_CA 1 GB\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:02,532] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 20.23 GB, percent = 10.8%\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:02,720] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:02,721] [INFO] [utils.py:782:see_memory_usage] MA 0.14 GB         Max_MA 0.14 GB         CA 1.3 GB         Max_CA 1 GB\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:02,721] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 20.24 GB, percent = 10.8%\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:03,339] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:03,339] [INFO] [utils.py:782:see_memory_usage] MA 0.14 GB         Max_MA 0.14 GB         CA 0.16 GB         Max_CA 1 GB\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:03,340] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 20.24 GB, percent = 10.8%\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:03,524] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:03,524] [INFO] [utils.py:782:see_memory_usage] MA 0.14 GB         Max_MA 0.14 GB         CA 0.16 GB         Max_CA 0 GB\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:03,525] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 20.24 GB, percent = 10.8%\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:03,712] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:03,712] [INFO] [utils.py:782:see_memory_usage] MA 0.42 GB         Max_MA 0.56 GB         CA 0.57 GB         Max_CA 1 GB\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:03,712] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 20.24 GB, percent = 10.8%\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:03,897] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:03,898] [INFO] [utils.py:782:see_memory_usage] MA 0.42 GB         Max_MA 0.42 GB         CA 0.57 GB         Max_CA 1 GB\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:03,898] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 20.24 GB, percent = 10.8%\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:04,084] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:04,084] [INFO] [utils.py:782:see_memory_usage] MA 0.42 GB         Max_MA 0.7 GB         CA 0.84 GB         Max_CA 1 GB\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:04,084] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 20.24 GB, percent = 10.8%\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:04,085] [INFO] [stage3.py:534:_setup_for_real_optimizer] optimizer state initialized\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,333] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,334] [INFO] [utils.py:782:see_memory_usage] MA 1.49 GB         Max_MA 1.63 GB         CA 1.72 GB         Max_CA 2 GB\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,334] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 21.23 GB, percent = 11.4%\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,334] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,334] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,334] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,334] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05, 3e-05], mom=[(0.9, 0.999), (0.9, 0.999)]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,336] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,336] [INFO] [config.py:921:print] DeepSpeedEngine configuration:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,336] [INFO] [config.py:925:print]   activation_checkpointing_config  {\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"partition_activations\": false, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"contiguous_memory_optimization\": false, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"cpu_checkpointing\": false, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"number_checkpoints\": null, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"synchronize_checkpoint_boundary\": false, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"profile\": false\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,336] [INFO] [config.py:925:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,336] [INFO] [config.py:925:print]   amp_enabled .................. False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,336] [INFO] [config.py:925:print]   amp_params ................... False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,336] [INFO] [config.py:925:print]   autotuning_config ............ {\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"enabled\": false, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"start_step\": null, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"end_step\": null, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"metric_path\": null, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"arg_mappings\": null, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"metric\": \"throughput\", \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"model_info\": null, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"results_dir\": \"autotuning_results\", \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"exps_dir\": \"autotuning_exps\", \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"overwrite\": true, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"fast\": true, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"start_profile_step\": 3, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"end_profile_step\": 5, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"tuner_type\": \"gridsearch\", \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"tuner_early_stopping\": 5, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"tuner_num_trials\": 50, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"model_info_path\": null, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"mp_size\": 1, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"max_train_batch_size\": null, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"min_train_batch_size\": 1, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"min_train_micro_batch_size_per_gpu\": 1, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"num_tuning_micro_batch_sizes\": 3\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,336] [INFO] [config.py:925:print]   bfloat16_config .............. enabled=True immediate_grad_update=False check_grad_overflow=False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   checkpoint_parallel_write_pipeline  False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   checkpoint_tag_validation_enabled  True\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   checkpoint_tag_validation_fail  False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff85f4f2890>\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   communication_data_type ...... None\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   curriculum_enabled_legacy .... False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   curriculum_params_legacy ..... False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   data_efficiency_enabled ...... False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   dataloader_drop_last ......... False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   disable_allgather ............ False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   dump_state ................... False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   eigenvalue_enabled ........... False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   eigenvalue_gas_boundary_resolution  1\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   eigenvalue_layer_num ......... 0\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   eigenvalue_max_iter .......... 100\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   eigenvalue_stability ......... 1e-06\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   eigenvalue_tol ............... 0.01\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   eigenvalue_verbose ........... False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   elasticity_enabled ........... False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,337] [INFO] [config.py:925:print]   float16_config ............... enabled=False auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   flops_profiler_config ........ {\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"enabled\": false, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"recompute_fwd_factor\": 0.0, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"profile_step\": 1, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"module_depth\": -1, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"top_modules\": 1, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"detailed\": true, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"output_file\": null\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   global_rank .................. 0\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   grad_accum_dtype ............. None\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   gradient_accumulation_steps .. 1\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   gradient_clipping ............ 0.0\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   gradient_predivide_factor .... 1.0\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   graph_harvesting ............. False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   load_universal_checkpoint .... False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   memory_breakdown ............. False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   mics_hierarchial_params_gather  False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   mics_shard_size .............. -1\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   nebula_config ................ {\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"enabled\": false, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"persistent_storage_path\": null, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"persistent_time_interval\": 100, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"num_of_version_in_retention\": 2, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"enable_nebula_load\": true, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"load_path\": null\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   optimizer_legacy_fusion ...... False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   optimizer_name ............... None\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   optimizer_params ............. None\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   pld_enabled .................. False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   pld_params ................... False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   prescale_gradients ........... False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   scheduler_name ............... None\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   scheduler_params ............. None\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   seq_parallel_communication_data_type  torch.float32\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   sparse_attention ............. None\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   sparse_gradients_enabled ..... False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   steps_per_print .............. inf\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   timers_config ................ enabled=True synchronized=True\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   train_batch_size ............. 32\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,338] [INFO] [config.py:925:print]   train_micro_batch_size_per_gpu  8\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,339] [INFO] [config.py:925:print]   use_data_before_expert_parallel_  False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,339] [INFO] [config.py:925:print]   use_node_local_storage ....... False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,339] [INFO] [config.py:925:print]   wall_clock_breakdown ......... False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,339] [INFO] [config.py:925:print]   weight_quantization_config ... None\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,339] [INFO] [config.py:925:print]   world_size ................... 4\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,339] [INFO] [config.py:925:print]   zero_allow_untested_optimizer  True\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,339] [INFO] [config.py:925:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=1000000 model_persistence_threshold=9223372036854775807 max_live_parameters=30000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,339] [INFO] [config.py:925:print]   zero_enabled ................. True\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,339] [INFO] [config.py:925:print]   zero_force_ds_cpu_optimizer .. True\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,339] [INFO] [config.py:925:print]   zero_optimization_stage ...... 3\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:42:05,339] [INFO] [config.py:911:print_user_config]   json = {\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"zero_optimization\": {\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:        \"stage\": 3, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:        \"overlap_comm\": true, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:        \"contiguous_gradients\": true, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:        \"reduce_scatter\": true, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:        \"allgather_partitions\": true, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:        \"stage3_param_persistence_threshold\": 1.000000e+06, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:        \"stage3_max_live_parameters\": 3.000000e+07\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    }, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"bf16\": {\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:        \"enabled\": true\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    }, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"train_micro_batch_size_per_gpu\": 8, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"gradient_accumulation_steps\": 1, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"train_batch_size\": 32, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"steps_per_print\": inf, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"fp16\": {\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:        \"enabled\": false\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    }, \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:    \"zero_allow_untested_optimizer\": true\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:***** Running training *****\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  Num examples = 3,689\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  Num Epochs = 5\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  Instantaneous batch size per device = 8\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:***** Running training *****\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  Num examples = 3,689\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  Num Epochs = 5\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  Instantaneous batch size per device = 8\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  Total train batch size (w. parallel, distributed & accumulation) = 32\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  Total train batch size (w. parallel, distributed & accumulation) = 32\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  Gradient Accumulation steps = 1\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  Total optimization steps = 575\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  Gradient Accumulation steps = 1\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  Total optimization steps = 575\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  Number of trainable parameters = 299,310,464\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  Number of trainable parameters = 299,310,464\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/575 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:[rank0]:W0730 12:42:12.148000 460 site-packages/torch/_dynamo/convert_frame.py:844] [3/8] torch._dynamo hit config.cache_size_limit (8)\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:[rank0]:W0730 12:42:12.148000 460 site-packages/torch/_dynamo/convert_frame.py:844] [3/8]    function: 'post_sub_module_forward_function' (/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py:477)\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:[rank0]:W0730 12:42:12.148000 460 site-packages/torch/_dynamo/convert_frame.py:844] [3/8]    last reason: 3/0: ___check_type_id(L['sub_module'], 94479487538016)           \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:[rank0]:W0730 12:42:12.148000 460 site-packages/torch/_dynamo/convert_frame.py:844] [3/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:[rank0]:W0730 12:42:12.148000 460 site-packages/torch/_dynamo/convert_frame.py:844] [3/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:[rank0]:W0730 12:42:12.149000 460 site-packages/torch/_dynamo/convert_frame.py:844] [11/8] torch._dynamo hit config.cache_size_limit (8)\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:[rank0]:W0730 12:42:12.149000 460 site-packages/torch/_dynamo/convert_frame.py:844] [11/8]    function: '_pre_backward_module_hook' (/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py:348)\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:[rank0]:W0730 12:42:12.149000 460 site-packages/torch/_dynamo/convert_frame.py:844] [11/8]    last reason: 11/0: ___check_type_id(L['module'], 94479487538016)               \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:[rank0]:W0730 12:42:12.149000 460 site-packages/torch/_dynamo/convert_frame.py:844] [11/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:[rank0]:W0730 12:42:12.149000 460 site-packages/torch/_dynamo/convert_frame.py:844] [11/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:[rank3]:W0730 12:42:12.150000 463 site-packages/torch/_dynamo/convert_frame.py:844] [3/8] torch._dynamo hit config.cache_size_limit (8)\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:[rank3]:W0730 12:42:12.150000 463 site-packages/torch/_dynamo/convert_frame.py:844] [3/8]    function: 'post_sub_module_forward_function' (/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py:477)\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:[rank3]:W0730 12:42:12.150000 463 site-packages/torch/_dynamo/convert_frame.py:844] [3/8]    last reason: 3/0: ___check_type_id(L['sub_module'], 93907678377840)           \u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:[rank3]:W0730 12:42:12.150000 463 site-packages/torch/_dynamo/convert_frame.py:844] [3/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:[rank3]:W0730 12:42:12.150000 463 site-packages/torch/_dynamo/convert_frame.py:844] [3/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:[rank1]:W0730 12:42:12.150000 461 site-packages/torch/_dynamo/convert_frame.py:844] [3/8] torch._dynamo hit config.cache_size_limit (8)\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:[rank1]:W0730 12:42:12.150000 461 site-packages/torch/_dynamo/convert_frame.py:844] [3/8]    function: 'post_sub_module_forward_function' (/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py:477)\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:[rank1]:W0730 12:42:12.150000 461 site-packages/torch/_dynamo/convert_frame.py:844] [3/8]    last reason: 3/0: ___check_type_id(L['sub_module'], 94320946816256)           \u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:[rank1]:W0730 12:42:12.150000 461 site-packages/torch/_dynamo/convert_frame.py:844] [3/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:[rank1]:W0730 12:42:12.150000 461 site-packages/torch/_dynamo/convert_frame.py:844] [3/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:[rank3]:W0730 12:42:12.151000 463 site-packages/torch/_dynamo/convert_frame.py:844] [11/8] torch._dynamo hit config.cache_size_limit (8)\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:[rank3]:W0730 12:42:12.151000 463 site-packages/torch/_dynamo/convert_frame.py:844] [11/8]    function: '_pre_backward_module_hook' (/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py:348)\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:[rank3]:W0730 12:42:12.151000 463 site-packages/torch/_dynamo/convert_frame.py:844] [11/8]    last reason: 11/0: ___check_type_id(L['module'], 93907678377840)               \u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:[rank3]:W0730 12:42:12.151000 463 site-packages/torch/_dynamo/convert_frame.py:844] [11/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:[rank3]:W0730 12:42:12.151000 463 site-packages/torch/_dynamo/convert_frame.py:844] [11/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:[rank1]:W0730 12:42:12.151000 461 site-packages/torch/_dynamo/convert_frame.py:844] [11/8] torch._dynamo hit config.cache_size_limit (8)\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:[rank1]:W0730 12:42:12.151000 461 site-packages/torch/_dynamo/convert_frame.py:844] [11/8]    function: '_pre_backward_module_hook' (/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py:348)\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:[rank1]:W0730 12:42:12.151000 461 site-packages/torch/_dynamo/convert_frame.py:844] [11/8]    last reason: 11/0: ___check_type_id(L['module'], 94320946816256)               \u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:[rank1]:W0730 12:42:12.151000 461 site-packages/torch/_dynamo/convert_frame.py:844] [11/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:[rank1]:W0730 12:42:12.151000 461 site-packages/torch/_dynamo/convert_frame.py:844] [11/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:[rank2]:W0730 12:42:12.153000 462 site-packages/torch/_dynamo/convert_frame.py:844] [3/8] torch._dynamo hit config.cache_size_limit (8)\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:[rank2]:W0730 12:42:12.153000 462 site-packages/torch/_dynamo/convert_frame.py:844] [3/8]    function: 'post_sub_module_forward_function' (/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py:477)\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:[rank2]:W0730 12:42:12.153000 462 site-packages/torch/_dynamo/convert_frame.py:844] [3/8]    last reason: 3/0: ___check_type_id(L['sub_module'], 94184777168816)           \u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:[rank2]:W0730 12:42:12.153000 462 site-packages/torch/_dynamo/convert_frame.py:844] [3/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:[rank2]:W0730 12:42:12.153000 462 site-packages/torch/_dynamo/convert_frame.py:844] [3/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:[rank2]:W0730 12:42:12.154000 462 site-packages/torch/_dynamo/convert_frame.py:844] [11/8] torch._dynamo hit config.cache_size_limit (8)\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:[rank2]:W0730 12:42:12.154000 462 site-packages/torch/_dynamo/convert_frame.py:844] [11/8]    function: '_pre_backward_module_hook' (/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py:348)\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:[rank2]:W0730 12:42:12.154000 462 site-packages/torch/_dynamo/convert_frame.py:844] [11/8]    last reason: 11/0: ___check_type_id(L['module'], 94184777168816)               \u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:[rank2]:W0730 12:42:12.154000 462 site-packages/torch/_dynamo/convert_frame.py:844] [11/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:[rank2]:W0730 12:42:12.154000 462 site-packages/torch/_dynamo/convert_frame.py:844] [11/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:[rank1]:W0730 12:42:13.772000 461 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:[rank1]:W0730 12:42:13.772000 461 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: '_post_forward_module_hook' (/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py:300)\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:[rank1]:W0730 12:42:13.772000 461 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: ___check_type_id(L['module'], 94320946816256)               \u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:[rank1]:W0730 12:42:13.772000 461 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:[rank1]:W0730 12:42:13.772000 461 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:[rank0]:W0730 12:42:13.781000 460 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:[rank0]:W0730 12:42:13.781000 460 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: '_post_forward_module_hook' (/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py:300)\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:[rank0]:W0730 12:42:13.781000 460 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: ___check_type_id(L['module'], 94479487538016)               \u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:[rank0]:W0730 12:42:13.781000 460 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:[rank0]:W0730 12:42:13.781000 460 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:[rank2]:W0730 12:42:13.791000 462 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:[rank2]:W0730 12:42:13.791000 462 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: '_post_forward_module_hook' (/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py:300)\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:[rank2]:W0730 12:42:13.791000 462 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: ___check_type_id(L['module'], 94184777168816)               \u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:[rank2]:W0730 12:42:13.791000 462 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:[rank2]:W0730 12:42:13.791000 462 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:[rank3]:W0730 12:42:13.798000 463 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:[rank3]:W0730 12:42:13.798000 463 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: '_post_forward_module_hook' (/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py:300)\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:[rank3]:W0730 12:42:13.798000 463 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: ___check_type_id(L['module'], 93907678377840)               \u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:[rank3]:W0730 12:42:13.798000 463 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:[rank3]:W0730 12:42:13.798000 463 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 1/575 [00:12<2:01:52, 12.74s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 2/575 [00:15<1:07:51,  7.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 3/575 [00:19<50:30,  5.30s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 4/575 [00:22<42:31,  4.47s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 5/575 [00:25<37:46,  3.98s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 6/575 [00:28<35:26,  3.74s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 7/575 [00:31<33:32,  3.54s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|▏         | 8/575 [00:34<32:21,  3.42s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 9/575 [00:38<31:30,  3.34s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 10/575 [00:41<30:52,  3.28s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 11/575 [00:44<30:33,  3.25s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 12/575 [00:47<30:08,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 13/575 [00:50<29:44,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 14/575 [00:53<29:35,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 15/575 [00:56<29:30,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 16/575 [01:00<29:23,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 17/575 [01:03<29:12,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 18/575 [01:06<28:52,  3.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 19/575 [01:09<28:54,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 20/575 [01:12<28:53,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▎         | 21/575 [01:15<28:49,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 22/575 [01:18<28:47,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 23/575 [01:21<28:36,  3.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 24/575 [01:24<28:27,  3.10s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 25/575 [01:27<28:14,  3.08s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  5%|▍         | 26/575 [01:31<28:13,  3.08s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  5%|▍         | 27/575 [01:34<28:07,  3.08s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  5%|▍         | 28/575 [01:37<28:12,  3.09s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  5%|▌         | 29/575 [01:40<28:27,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  5%|▌         | 30/575 [01:43<28:26,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  5%|▌         | 31/575 [01:46<28:29,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 32/575 [01:49<28:21,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 33/575 [01:52<28:18,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 34/575 [01:56<28:17,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 35/575 [01:59<28:07,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▋         | 36/575 [02:02<28:12,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▋         | 37/575 [02:05<28:05,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 38/575 [02:08<28:03,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 39/575 [02:11<28:08,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 40/575 [02:15<28:09,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 41/575 [02:18<28:01,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 42/575 [02:21<27:59,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 43/575 [02:24<27:54,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 44/575 [02:27<28:00,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 45/575 [02:30<27:46,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 46/575 [02:33<27:39,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 47/575 [02:37<27:43,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 48/575 [02:40<27:42,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▊         | 49/575 [02:43<27:26,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▊         | 50/575 [02:46<27:27,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                                #015[1,mpirank:0,algo-1]<stderr>:#015  9%|▊         | 50/575 [02:46<27:27,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 3.3732, 'grad_norm': 207.89186595500075, 'learning_rate': 2.7443478260869565e-05, 'epoch': 0.43}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 51/575 [02:49<27:28,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 52/575 [02:52<27:20,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 53/575 [02:55<27:20,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 54/575 [02:58<27:08,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|▉         | 55/575 [03:02<27:11,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|▉         | 56/575 [03:05<27:09,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|▉         | 57/575 [03:08<27:10,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|█         | 58/575 [03:11<27:07,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|█         | 59/575 [03:14<27:04,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|█         | 60/575 [03:17<26:59,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 61/575 [03:20<26:45,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 62/575 [03:24<26:38,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 63/575 [03:27<26:39,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 64/575 [03:30<26:46,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█▏        | 65/575 [03:33<26:47,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█▏        | 66/575 [03:36<26:51,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 67/575 [03:39<26:40,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 68/575 [03:42<26:34,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 69/575 [03:46<26:32,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 70/575 [03:49<26:26,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 71/575 [03:52<26:23,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 72/575 [03:55<26:24,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 73/575 [03:58<26:18,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 74/575 [04:01<26:11,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 75/575 [04:04<26:09,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 76/575 [04:08<26:09,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 77/575 [04:11<26:17,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▎        | 78/575 [04:14<26:10,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▎        | 79/575 [04:17<26:12,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 80/575 [04:20<26:01,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 81/575 [04:23<25:56,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 82/575 [04:27<25:42,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 83/575 [04:30<25:31,  3.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▍        | 84/575 [04:33<25:40,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▍        | 85/575 [04:36<25:40,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▍        | 86/575 [04:39<25:32,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▌        | 87/575 [04:42<25:26,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▌        | 88/575 [04:45<25:25,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▌        | 89/575 [04:48<25:24,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 90/575 [04:52<25:16,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 91/575 [04:55<25:17,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 92/575 [04:58<25:33,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 93/575 [05:01<25:33,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▋        | 94/575 [05:04<25:25,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 95/575 [05:07<25:17,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 96/575 [05:11<25:14,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 97/575 [05:14<25:06,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 98/575 [05:17<24:57,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 99/575 [05:20<24:57,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 100/575 [05:23<24:56,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                                 #015[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 100/575 [05:23<24:56,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 2.7757, 'grad_norm': 7.105736666147374, 'learning_rate': 2.4834782608695652e-05, 'epoch': 0.87}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 101/575 [05:26<24:44,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 102/575 [05:29<24:40,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 103/575 [05:33<24:38,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 104/575 [05:36<24:38,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 105/575 [05:39<24:41,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 106/575 [05:42<24:30,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▊        | 107/575 [05:45<24:29,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 108/575 [05:48<24:31,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 109/575 [05:52<24:32,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 110/575 [05:55<24:23,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 111/575 [05:58<24:45,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 112/575 [06:01<24:24,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|█▉        | 113/575 [06:04<24:28,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|█▉        | 114/575 [06:07<24:14,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 115/575 [06:10<23:52,  3.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:***** Running Evaluation *****\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  Num examples = 483\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  Batch size = 8\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:***** Running Evaluation *****\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  Num examples = 483\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  Batch size = 8\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) [1,mpirank:2,algo-1]<stdout>:Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/15 [00:00<?, ?it/s]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 2/15 [00:01<00:10,  1.27it/s]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 3/15 [00:03<00:13,  1.10s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 4/15 [00:04<00:14,  1.29s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval [1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval [1,mpirank:3,algo-1]<stdout>:torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 5/15 [00:06<00:13,  1.34s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 6/15 [00:07<00:12,  1.38s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 7/15 [00:09<00:11,  1.40s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 8/15 [00:10<00:09,  1.40s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 9/15 [00:11<00:08,  1.38s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 10/15 [00:13<00:06,  1.36s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 11/15 [00:14<00:05,  1.36s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32])[1,mpirank:3,algo-1]<stdout>: Labels eval torch.Size([8])[1,mpirank:3,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 12/15 [00:15<00:04,  1.41s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 13/15 [00:17<00:02,  1.44s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval [1,mpirank:0,algo-1]<stdout>:torch.Size([8])[1,mpirank:0,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 14/15 [00:18<00:01,  1.44s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 15/15 [00:20<00:00,  1.39s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                                 #015\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                               #015#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'eval_accuracy': 0.03125, 'eval_precision': 0.040554780850781326, 'eval_recall': 0.03125, 'eval_f1': 0.01645306305172011, 'epoch': 1.0}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 115/575 [06:32<23:52,  3.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 15/15 [00:20<00:00,  1.39s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                                 #015\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                               #015#033[A[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 115/575 [06:32<23:52,  3.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'eval_accuracy': 0.03125, 'eval_precision': 0.040554780850781326, 'eval_recall': 0.03125, 'eval_f1': 0.01645306305172011, 'eval_epoch': 1.0, 'eval_runtime': 21.7925, 'eval_samples_per_second': 22.164, 'eval_steps_per_second': 0.734, 'epoch': 1.0}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 15/15 [00:20<00:00,  1.39s/it]#033[A[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                               #033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Saving model checkpoint to /opt/ml/model/checkpoint-115\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Saving model checkpoint to /opt/ml/model/checkpoint-115\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Trainer.model is not a `PreTrainedModel`, only saving its state dict.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Trainer.model is not a `PreTrainedModel`, only saving its state dict.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:48:38,094] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step115 is begin to save!\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:48:38,104] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /opt/ml/model/checkpoint-115/global_step115/zero_pp_rank_0_mp_rank_00_model_states.pt\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:48:40,179] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step115 is begin to save!\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:48:40,190] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /opt/ml/model/checkpoint-115/global_step115/zero_pp_rank_0_mp_rank_00_model_states.pt\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 116/575 [06:42<1:28:17, 11.54s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 117/575 [06:45<1:08:47,  9.01s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 118/575 [06:48<55:15,  7.26s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 119/575 [06:51<45:48,  6.03s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 120/575 [06:54<39:12,  5.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 121/575 [06:57<34:29,  4.56s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 122/575 [07:00<31:13,  4.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██▏       | 123/575 [07:04<28:55,  3.84s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 124/575 [07:07<27:13,  3.62s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 125/575 [07:10<25:59,  3.47s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 126/575 [07:13<25:07,  3.36s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 127/575 [07:16<24:31,  3.28s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 128/575 [07:19<24:03,  3.23s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 129/575 [07:22<23:49,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 130/575 [07:25<23:37,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 131/575 [07:29<23:32,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 132/575 [07:32<23:11,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 133/575 [07:35<23:08,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 134/575 [07:38<22:55,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 135/575 [07:41<23:00,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▎       | 136/575 [07:44<23:07,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 137/575 [07:47<22:49,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 138/575 [07:50<22:49,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 139/575 [07:54<22:47,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 140/575 [07:57<22:50,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▍       | 141/575 [08:00<22:37,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▍       | 142/575 [08:03<22:34,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▍       | 143/575 [08:06<22:35,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▌       | 144/575 [08:09<22:36,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▌       | 145/575 [08:12<22:33,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▌       | 146/575 [08:15<22:14,  3.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 147/575 [08:19<22:09,  3.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 148/575 [08:22<22:04,  3.10s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 149/575 [08:25<22:06,  3.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 150/575 [08:28<22:04,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 2.2296, 'grad_norm': 10.391663188789149, 'learning_rate': 2.222608695652174e-05, 'epoch': 1.3}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 150/575 [08:28<22:04,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▋       | 151/575 [08:31<22:05,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▋       | 152/575 [08:34<22:03,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 153/575 [08:37<21:54,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 154/575 [08:40<21:54,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 155/575 [08:44<21:54,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 156/575 [08:47<21:53,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 157/575 [08:50<21:46,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 158/575 [08:53<21:43,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 159/575 [08:56<21:42,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 160/575 [08:59<21:42,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 161/575 [09:02<21:39,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 162/575 [09:05<21:30,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 163/575 [09:09<21:20,  3.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▊       | 164/575 [09:12<21:25,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▊       | 165/575 [09:15<21:24,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 166/575 [09:18<21:19,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 167/575 [09:21<21:24,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 168/575 [09:24<21:14,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 169/575 [09:27<21:13,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|██▉       | 170/575 [09:31<21:08,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|██▉       | 171/575 [09:34<21:13,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|██▉       | 172/575 [09:37<21:06,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|███       | 173/575 [09:40<21:02,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|███       | 174/575 [09:43<21:00,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|███       | 175/575 [09:46<20:52,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 176/575 [09:49<20:42,  3.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 177/575 [09:52<20:39,  3.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 178/575 [09:56<20:44,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 179/575 [09:59<20:42,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███▏      | 180/575 [10:02<20:41,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███▏      | 181/575 [10:05<20:35,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 182/575 [10:08<20:29,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 183/575 [10:11<20:34,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 184/575 [10:15<20:33,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 185/575 [10:18<20:28,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 186/575 [10:21<20:20,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 187/575 [10:24<20:09,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 188/575 [10:27<20:09,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 189/575 [10:30<20:08,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 190/575 [10:33<20:05,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 191/575 [10:36<20:03,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 192/575 [10:40<20:07,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▎      | 193/575 [10:43<20:00,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▎      | 194/575 [10:46<20:01,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 195/575 [10:49<19:50,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 196/575 [10:52<19:41,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 197/575 [10:55<19:36,  3.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 198/575 [10:58<19:30,  3.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▍      | 199/575 [11:01<19:24,  3.10s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▍      | 200/575 [11:04<19:20,  3.09s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 1.5484, 'grad_norm': 8.177021628272167, 'learning_rate': 1.9617391304347827e-05, 'epoch': 1.74}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▍      | 200/575 [11:04<19:20,  3.09s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▍      | 201/575 [11:08<19:28,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▌      | 202/575 [11:11<19:20,  3.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▌      | 203/575 [11:14<19:16,  3.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▌      | 204/575 [11:17<19:17,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 205/575 [11:20<19:45,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 206/575 [11:23<19:31,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 207/575 [11:27<19:23,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 208/575 [11:30<19:23,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▋      | 209/575 [11:33<19:12,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 210/575 [11:36<19:15,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 211/575 [11:39<19:06,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 212/575 [11:42<19:10,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 213/575 [11:46<19:03,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 214/575 [11:49<18:52,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 215/575 [11:52<18:44,  3.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 216/575 [11:55<18:43,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 217/575 [11:58<18:50,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 218/575 [12:01<18:40,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 219/575 [12:04<18:37,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 220/575 [12:07<18:34,  3.14s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 221/575 [12:11<18:47,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▊      | 222/575 [12:14<18:41,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 223/575 [12:17<18:34,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 224/575 [12:20<18:33,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 225/575 [12:23<18:32,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 226/575 [12:27<18:30,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 227/575 [12:30<18:17,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|███▉      | 228/575 [12:33<18:19,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|███▉      | 229/575 [12:36<18:18,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 230/575 [12:39<18:00,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:***** Running Evaluation *****\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  Num examples = 483\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  Batch size = 8\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:***** Running Evaluation *****\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  Num examples = 483\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  Batch size = 8\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/15 [00:00<?, ?it/s]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32])[1,mpirank:1,algo-1]<stdout>: Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 2/15 [00:01<00:09,  1.31it/s]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) [1,mpirank:1,algo-1]<stdout>:Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 3/15 [00:03<00:13,  1.11s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 4/15 [00:04<00:14,  1.29s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 5/15 [00:06<00:13,  1.37s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 6/15 [00:07<00:12,  1.40s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 7/15 [00:09<00:11,  1.43s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 8/15 [00:10<00:09,  1.41s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval [1,mpirank:3,algo-1]<stdout>:torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 9/15 [00:11<00:08,  1.40s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval [1,mpirank:3,algo-1]<stdout>:torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 10/15 [00:13<00:06,  1.37s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 11/15 [00:14<00:05,  1.38s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 12/15 [00:16<00:04,  1.41s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 13/15 [00:17<00:02,  1.43s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval [1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: [1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])[1,mpirank:2,algo-1]<stdout>:torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 14/15 [00:19<00:01,  1.44s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 15/15 [00:20<00:00,  1.40s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                                 #015\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                               #015#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'eval_accuracy': 0.035416666666666666, 'eval_precision': 0.06301256189475694, 'eval_recall': 0.035416666666666666, 'eval_f1': 0.027901647187335937, 'epoch': 2.0}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 230/575 [13:01<18:00,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 15/15 [00:20<00:00,  1.40s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                                 #015\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                               #015#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'eval_accuracy': 0.035416666666666666, 'eval_precision': 0.06301256189475694, 'eval_recall': 0.035416666666666666, 'eval_f1': 0.027901647187335937, 'eval_epoch': 2.0, 'eval_runtime': 22.0633, 'eval_samples_per_second': 21.892, 'eval_steps_per_second': 0.725, 'epoch': 2.0}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 230/575 [13:01<18:00,  3.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 15/15 [00:20<00:00,  1.40s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                               #033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Saving model checkpoint to /opt/ml/model/checkpoint-230\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Saving model checkpoint to /opt/ml/model/checkpoint-230\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Trainer.model is not a `PreTrainedModel`, only saving its state dict.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Trainer.model is not a `PreTrainedModel`, only saving its state dict.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:55:07,027] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step230 is begin to save!\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:55:07,036] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /opt/ml/model/checkpoint-230/global_step230/zero_pp_rank_0_mp_rank_00_model_states.pt\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:55:09,130] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step230 is begin to save!\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 12:55:09,140] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /opt/ml/model/checkpoint-230/global_step230/zero_pp_rank_0_mp_rank_00_model_states.pt\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 231/575 [13:11<1:06:55, 11.67s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 232/575 [13:14<52:11,  9.13s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 233/575 [13:17<41:49,  7.34s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 234/575 [13:20<34:42,  6.11s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 235/575 [13:24<29:46,  5.25s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 236/575 [13:27<26:11,  4.64s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 237/575 [13:30<23:37,  4.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████▏     | 238/575 [13:33<21:48,  3.88s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 239/575 [13:36<20:39,  3.69s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 240/575 [13:39<19:40,  3.52s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 241/575 [13:43<19:02,  3.42s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 242/575 [13:46<18:31,  3.34s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 243/575 [13:49<18:03,  3.26s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 244/575 [13:52<17:50,  3.23s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 245/575 [13:55<17:44,  3.22s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 246/575 [13:58<17:33,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 247/575 [14:01<17:18,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 248/575 [14:05<17:11,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 249/575 [14:08<17:06,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 250/575 [14:11<17:04,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 1.1782, 'grad_norm': 6.617476019492443, 'learning_rate': 1.7008695652173914e-05, 'epoch': 2.17}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 250/575 [14:11<17:04,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▎     | 251/575 [14:14<17:13,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 252/575 [14:17<17:06,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 253/575 [14:20<17:02,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 254/575 [14:24<16:54,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 255/575 [14:27<16:59,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▍     | 256/575 [14:30<16:58,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▍     | 257/575 [14:33<17:00,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▍     | 258/575 [14:36<16:50,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▌     | 259/575 [14:40<16:43,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▌     | 260/575 [14:43<16:41,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▌     | 261/575 [14:46<16:33,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 262/575 [14:49<16:32,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 263/575 [14:52<16:29,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 264/575 [14:55<16:30,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 265/575 [14:59<16:28,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▋     | 266/575 [15:02<16:27,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▋     | 267/575 [15:05<16:17,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 268/575 [15:08<16:13,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 269/575 [15:11<16:12,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 270/575 [15:15<16:09,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 271/575 [15:18<16:15,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 272/575 [15:21<16:13,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 273/575 [15:24<16:07,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 274/575 [15:27<16:04,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 275/575 [15:31<16:05,  3.22s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 276/575 [15:34<15:56,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 277/575 [15:37<15:48,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 278/575 [15:40<15:42,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▊     | 279/575 [15:43<15:42,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▊     | 280/575 [15:46<15:34,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 281/575 [15:50<15:36,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 282/575 [15:53<15:33,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 283/575 [15:56<15:27,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 284/575 [15:59<15:26,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|████▉     | 285/575 [16:02<15:18,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|████▉     | 286/575 [16:06<15:14,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|████▉     | 287/575 [16:09<15:09,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|█████     | 288/575 [16:12<15:09,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|█████     | 289/575 [16:15<15:28,  3.25s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|█████     | 290/575 [16:18<15:16,  3.22s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 291/575 [16:22<15:11,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 292/575 [16:25<15:03,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 293/575 [16:28<14:57,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 294/575 [16:31<14:47,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████▏    | 295/575 [16:34<14:43,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████▏    | 296/575 [16:37<14:45,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 297/575 [16:41<14:47,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 298/575 [16:44<14:47,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 299/575 [16:47<14:41,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 300/575 [16:50<14:39,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 300/575 [16:50<14:39,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 0.8793, 'grad_norm': 9.46411082636728, 'learning_rate': 1.44e-05, 'epoch': 2.61}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 301/575 [16:53<14:36,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 302/575 [16:57<14:30,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 303/575 [17:00<14:26,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 304/575 [17:03<14:23,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 305/575 [17:06<14:21,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 306/575 [17:09<14:17,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 307/575 [17:13<14:20,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▎    | 308/575 [17:16<14:14,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▎    | 309/575 [17:19<14:05,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 310/575 [17:22<14:01,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 311/575 [17:25<14:00,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 312/575 [17:29<13:59,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 313/575 [17:32<13:51,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▍    | 314/575 [17:35<13:48,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▍    | 315/575 [17:38<13:47,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▍    | 316/575 [17:41<13:43,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▌    | 317/575 [17:44<13:43,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▌    | 318/575 [17:48<13:37,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▌    | 319/575 [17:51<13:37,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 320/575 [17:54<13:26,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 321/575 [17:57<13:26,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 322/575 [18:00<13:28,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 323/575 [18:04<13:35,  3.24s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▋    | 324/575 [18:07<13:35,  3.25s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 325/575 [18:10<13:28,  3.24s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 326/575 [18:13<13:17,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 327/575 [18:16<13:11,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 328/575 [18:20<13:11,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 329/575 [18:23<13:10,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 330/575 [18:26<13:08,  3.22s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 331/575 [18:29<13:03,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 332/575 [18:32<12:55,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 333/575 [18:36<12:53,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 334/575 [18:39<12:52,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 335/575 [18:42<12:51,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 336/575 [18:45<12:42,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▊    | 337/575 [18:48<12:40,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 338/575 [18:52<12:36,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 339/575 [18:55<12:30,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 340/575 [18:58<12:27,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 341/575 [19:01<12:27,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 342/575 [19:05<12:32,  3.23s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|█████▉    | 343/575 [19:08<12:27,  3.22s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|█████▉    | 344/575 [19:11<12:19,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 345/575 [19:14<12:08,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:***** Running Evaluation *****\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:***** Running Evaluation *****\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  Num examples = 483\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  Batch size = 8\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  Num examples = 483\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  Batch size = 8\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/15 [00:00<?, ?it/s]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 2/15 [00:01<00:10,  1.27it/s]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 3/15 [00:03<00:13,  1.10s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval[1,mpirank:1,algo-1]<stdout>:Logits shape eval:[1,mpirank:3,algo-1]<stdout>: torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:torch.Size([8, 32])[1,mpirank:1,algo-1]<stdout>: Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 4/15 [00:04<00:14,  1.28s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 5/15 [00:06<00:13,  1.34s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 6/15 [00:07<00:12,  1.38s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 7/15 [00:09<00:11,  1.41s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])[1,mpirank:1,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 8/15 [00:10<00:09,  1.41s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 9/15 [00:11<00:08,  1.39s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 10/15 [00:13<00:06,  1.36s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) [1,mpirank:1,algo-1]<stdout>:Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 11/15 [00:14<00:05,  1.36s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>: torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])[1,mpirank:3,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 12/15 [00:15<00:04,  1.39s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 13/15 [00:17<00:02,  1.41s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval [1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 14/15 [00:18<00:01,  1.43s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 15/15 [00:20<00:00,  1.40s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                                 #015\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                               #015#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'eval_accuracy': 0.03958333333333333, 'eval_precision': 0.05884674278016669, 'eval_recall': 0.03958333333333333, 'eval_f1': 0.034054422024127715, 'epoch': 3.0}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 345/575 [19:36<12:08,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 15/15 [00:20<00:00,  1.40s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                                 #015\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                               #015#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'eval_accuracy': 0.03958333333333333, 'eval_precision': 0.05884674278016669, 'eval_recall': 0.03958333333333333, 'eval_f1': 0.034054422024127715, 'eval_epoch': 3.0, 'eval_runtime': 21.9077, 'eval_samples_per_second': 22.047, 'eval_steps_per_second': 0.73, 'epoch': 3.0}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 345/575 [19:36<12:08,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 15/15 [00:20<00:00,  1.40s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                               #033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Saving model checkpoint to /opt/ml/model/checkpoint-345\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Saving model checkpoint to /opt/ml/model/checkpoint-345\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Trainer.model is not a `PreTrainedModel`, only saving its state dict.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Trainer.model is not a `PreTrainedModel`, only saving its state dict.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 13:01:41,762] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step345 is begin to save!\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 13:01:41,771] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /opt/ml/model/checkpoint-345/global_step345/zero_pp_rank_0_mp_rank_00_model_states.pt\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 13:01:43,867] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step345 is begin to save!\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 13:01:43,877] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /opt/ml/model/checkpoint-345/global_step345/zero_pp_rank_0_mp_rank_00_model_states.pt\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 346/575 [19:45<44:27, 11.65s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 347/575 [19:49<34:34,  9.10s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 348/575 [19:52<27:42,  7.33s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 349/575 [19:55<22:52,  6.07s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 350/575 [19:58<19:28,  5.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 0.8056, 'grad_norm': 5.864212781342744, 'learning_rate': 1.1791304347826087e-05, 'epoch': 3.04}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 350/575 [19:58<19:28,  5.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 351/575 [20:01<17:09,  4.60s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 352/575 [20:05<15:36,  4.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████▏   | 353/575 [20:08<14:23,  3.89s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 354/575 [20:11<13:36,  3.69s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 355/575 [20:14<12:58,  3.54s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 356/575 [20:17<12:28,  3.42s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 357/575 [20:20<12:06,  3.33s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 358/575 [20:24<11:57,  3.31s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 359/575 [20:27<11:47,  3.28s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 360/575 [20:30<11:35,  3.23s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 361/575 [20:33<11:27,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 362/575 [20:36<11:23,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 363/575 [20:39<11:14,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 364/575 [20:43<11:12,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 365/575 [20:46<11:03,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▎   | 366/575 [20:49<11:02,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 367/575 [20:52<11:00,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 368/575 [20:55<10:57,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 369/575 [20:58<10:50,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 370/575 [21:02<10:49,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▍   | 371/575 [21:05<10:52,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▍   | 372/575 [21:08<10:45,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▍   | 373/575 [21:11<10:39,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▌   | 374/575 [21:14<10:35,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▌   | 375/575 [21:17<10:29,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▌   | 376/575 [21:21<10:31,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 377/575 [21:24<10:31,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 378/575 [21:27<10:28,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 379/575 [21:30<10:22,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 380/575 [21:33<10:17,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▋   | 381/575 [21:37<10:13,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▋   | 382/575 [21:40<10:11,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 383/575 [21:43<10:06,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 384/575 [21:46<10:03,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 385/575 [21:49<10:04,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 386/575 [21:52<10:02,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 387/575 [21:56<10:01,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 388/575 [21:59<09:56,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 389/575 [22:02<09:56,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 390/575 [22:05<09:54,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 391/575 [22:08<09:49,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 392/575 [22:12<09:50,  3.23s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 393/575 [22:15<09:45,  3.22s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▊   | 394/575 [22:18<09:53,  3.28s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▊   | 395/575 [22:22<09:45,  3.25s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 396/575 [22:25<09:33,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 397/575 [22:28<09:31,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 398/575 [22:31<09:28,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 399/575 [22:34<09:27,  3.23s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|██████▉   | 400/575 [22:38<09:22,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 0.5908, 'grad_norm': 6.093016367824986, 'learning_rate': 9.182608695652174e-06, 'epoch': 3.48}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|██████▉   | 400/575 [22:38<09:22,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|██████▉   | 401/575 [22:41<09:17,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|██████▉   | 402/575 [22:44<09:14,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|███████   | 403/575 [22:47<09:05,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|███████   | 404/575 [22:50<09:02,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|███████   | 405/575 [22:53<08:59,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████   | 406/575 [22:57<08:56,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████   | 407/575 [23:00<08:55,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████   | 408/575 [23:03<08:50,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████   | 409/575 [23:06<08:50,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████▏  | 410/575 [23:09<08:46,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████▏  | 411/575 [23:12<08:40,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 412/575 [23:16<08:41,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 413/575 [23:19<08:34,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 414/575 [23:22<08:32,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 415/575 [23:25<08:26,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 416/575 [23:28<08:21,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 417/575 [23:32<08:20,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 418/575 [23:35<08:15,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 419/575 [23:38<08:15,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 420/575 [23:41<08:10,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 421/575 [23:44<08:07,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 422/575 [23:47<08:03,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▎  | 423/575 [23:50<08:00,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▎  | 424/575 [23:54<07:58,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 425/575 [23:57<07:57,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 426/575 [24:00<07:51,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 427/575 [24:03<07:49,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 428/575 [24:06<07:46,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▍  | 429/575 [24:10<07:43,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▍  | 430/575 [24:13<07:42,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▍  | 431/575 [24:16<07:36,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▌  | 432/575 [24:19<07:34,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▌  | 433/575 [24:22<07:33,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▌  | 434/575 [24:26<07:29,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 435/575 [24:29<07:26,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 436/575 [24:32<07:24,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 437/575 [24:35<07:21,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 438/575 [24:38<07:21,  3.22s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▋  | 439/575 [24:42<07:16,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 440/575 [24:45<07:11,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 441/575 [24:48<07:06,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 442/575 [24:51<07:04,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 443/575 [24:54<07:01,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 444/575 [24:58<06:58,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 445/575 [25:01<06:54,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 446/575 [25:04<06:50,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 447/575 [25:07<06:47,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 448/575 [25:10<06:44,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 449/575 [25:13<06:43,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 450/575 [25:17<06:40,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 0.5595, 'grad_norm': 6.492317451478108, 'learning_rate': 6.573913043478261e-06, 'epoch': 3.91}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 450/575 [25:17<06:40,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 451/575 [25:20<06:37,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▊  | 452/575 [25:23<06:33,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 453/575 [25:26<06:28,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 454/575 [25:29<06:25,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 455/575 [25:33<06:22,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 456/575 [25:36<06:17,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 457/575 [25:39<06:15,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|███████▉  | 458/575 [25:42<06:11,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|███████▉  | 459/575 [25:45<06:10,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 460/575 [25:48<06:03,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:***** Running Evaluation *****\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  Num examples = 483\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  Batch size = 8\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:***** Running Evaluation *****\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  Num examples = 483\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  Batch size = 8\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/15 [00:00<?, ?it/s]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 2/15 [00:01<00:10,  1.29it/s]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 3/15 [00:03<00:12,  1.08s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) [1,mpirank:0,algo-1]<stdout>:Logits shape eval:[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) [1,mpirank:2,algo-1]<stdout>:Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Labels eval [1,mpirank:1,algo-1]<stdout>:torch.Size([8])[1,mpirank:1,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 4/15 [00:04<00:13,  1.27s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 5/15 [00:06<00:13,  1.33s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32])[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: [1,mpirank:0,algo-1]<stdout>: Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 6/15 [00:07<00:12,  1.39s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 7/15 [00:09<00:11,  1.42s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 8/15 [00:10<00:09,  1.41s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 9/15 [00:11<00:08,  1.39s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 10/15 [00:13<00:06,  1.36s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 11/15 [00:14<00:05,  1.37s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 12/15 [00:15<00:04,  1.40s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 13/15 [00:17<00:02,  1.43s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 14/15 [00:18<00:01,  1.46s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32])[1,mpirank:1,algo-1]<stdout>: Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 15/15 [00:20<00:00,  1.42s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                                 #015\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                               #015#033[A[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 460/575 [26:10<06:03,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'eval_accuracy': 0.03958333333333333, 'eval_precision': 0.05020253687332308, 'eval_recall': 0.03958333333333333, 'eval_f1': 0.03436444501941896, 'epoch': 4.0}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 15/15 [00:20<00:00,  1.42s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                                 #015\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#033[A[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 460/575 [26:10<06:03,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'eval_accuracy': 0.03958333333333333, 'eval_precision': 0.05020253687332308, 'eval_recall': 0.03958333333333333, 'eval_f1': 0.03436444501941896, 'eval_epoch': 4.0, 'eval_runtime': 21.9151, 'eval_samples_per_second': 22.04, 'eval_steps_per_second': 0.73, 'epoch': 4.0}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 15/15 [00:20<00:00,  1.42s/it]#033[A[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                               #033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Saving model checkpoint to /opt/ml/model/checkpoint-460\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Saving model checkpoint to /opt/ml/model/checkpoint-460\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Trainer.model is not a `PreTrainedModel`, only saving its state dict.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Trainer.model is not a `PreTrainedModel`, only saving its state dict.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 13:08:16,241] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step460 is begin to save!\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 13:08:16,250] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /opt/ml/model/checkpoint-460/global_step460/zero_pp_rank_0_mp_rank_00_model_states.pt\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 13:08:18,350] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step460 is begin to save!\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 13:08:18,360] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /opt/ml/model/checkpoint-460/global_step460/zero_pp_rank_0_mp_rank_00_model_states.pt\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 461/575 [26:20<22:07, 11.64s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 462/575 [26:23<17:10,  9.12s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 463/575 [26:26<13:39,  7.32s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 464/575 [26:29<11:13,  6.07s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 465/575 [26:33<09:34,  5.22s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 466/575 [26:36<08:22,  4.61s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 467/575 [26:39<07:31,  4.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████▏ | 468/575 [26:42<06:54,  3.87s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 469/575 [26:45<06:29,  3.67s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 470/575 [26:49<06:11,  3.53s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 471/575 [26:52<05:55,  3.42s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 472/575 [26:55<05:45,  3.35s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 473/575 [26:58<05:38,  3.32s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 474/575 [27:01<05:29,  3.26s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 475/575 [27:04<05:23,  3.24s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 476/575 [27:08<05:17,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 477/575 [27:11<05:13,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 478/575 [27:14<05:11,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 479/575 [27:17<05:07,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 480/575 [27:20<05:04,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▎ | 481/575 [27:24<05:01,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 482/575 [27:27<05:00,  3.23s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 483/575 [27:30<04:56,  3.22s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 484/575 [27:33<04:50,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 485/575 [27:36<04:49,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▍ | 486/575 [27:40<04:52,  3.28s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▍ | 487/575 [27:43<04:45,  3.25s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▍ | 488/575 [27:46<04:41,  3.24s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▌ | 489/575 [27:49<04:37,  3.22s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▌ | 490/575 [27:53<04:34,  3.23s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▌ | 491/575 [27:56<04:31,  3.23s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 492/575 [27:59<04:28,  3.23s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 493/575 [28:02<04:23,  3.22s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 494/575 [28:06<04:19,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 495/575 [28:09<04:15,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▋ | 496/575 [28:12<04:12,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▋ | 497/575 [28:15<04:09,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 498/575 [28:18<04:04,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 499/575 [28:21<04:02,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 500/575 [28:25<03:58,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                                 #015[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 500/575 [28:25<03:58,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 0.4435, 'grad_norm': 4.925781110525837, 'learning_rate': 3.965217391304348e-06, 'epoch': 4.35}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 501/575 [28:28<03:56,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 502/575 [28:31<03:55,  3.22s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 503/575 [28:34<03:50,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 504/575 [28:37<03:47,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 505/575 [28:41<03:44,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 506/575 [28:44<03:40,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 507/575 [28:47<03:35,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 508/575 [28:50<03:32,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▊ | 509/575 [28:53<03:29,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▊ | 510/575 [28:57<03:27,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 511/575 [29:00<03:25,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 512/575 [29:03<03:21,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 513/575 [29:06<03:18,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 514/575 [29:09<03:15,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|████████▉ | 515/575 [29:13<03:12,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|████████▉ | 516/575 [29:16<03:08,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|████████▉ | 517/575 [29:19<03:04,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|█████████ | 518/575 [29:22<03:02,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|█████████ | 519/575 [29:25<02:58,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|█████████ | 520/575 [29:28<02:54,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 521/575 [29:32<02:50,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 522/575 [29:35<02:47,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 523/575 [29:38<02:44,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 524/575 [29:41<02:42,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████▏| 525/575 [29:44<02:39,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████▏| 526/575 [29:48<02:36,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 527/575 [29:51<02:32,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 528/575 [29:54<02:29,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 529/575 [29:57<02:26,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 530/575 [30:00<02:23,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 531/575 [30:04<02:20,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 532/575 [30:07<02:17,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 533/575 [30:10<02:13,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 534/575 [30:13<02:11,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 535/575 [30:16<02:07,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 536/575 [30:19<02:04,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 537/575 [30:23<02:01,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▎| 538/575 [30:26<01:58,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▎| 539/575 [30:29<01:55,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 540/575 [30:32<01:52,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 541/575 [30:36<01:48,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 542/575 [30:39<01:46,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 543/575 [30:42<01:42,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▍| 544/575 [30:45<01:38,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▍| 545/575 [30:48<01:35,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▍| 546/575 [30:52<01:32,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▌| 547/575 [30:55<01:29,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▌| 548/575 [30:58<01:26,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▌| 549/575 [31:01<01:22,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 550/575 [31:04<01:19,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 0.4161, 'grad_norm': 6.595960945076221, 'learning_rate': 1.3565217391304349e-06, 'epoch': 4.78}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 550/575 [31:04<01:19,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 551/575 [31:07<01:16,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 552/575 [31:11<01:12,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 553/575 [31:14<01:10,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▋| 554/575 [31:17<01:07,  3.21s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 555/575 [31:20<01:03,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 556/575 [31:23<01:00,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 557/575 [31:26<00:57,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 558/575 [31:30<00:53,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 559/575 [31:33<00:50,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 560/575 [31:36<00:47,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 561/575 [31:39<00:44,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 562/575 [31:42<00:41,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 563/575 [31:45<00:37,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 564/575 [31:49<00:34,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 565/575 [31:52<00:31,  3.15s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 566/575 [31:55<00:28,  3.16s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▊| 567/575 [31:58<00:25,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 568/575 [32:01<00:22,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 569/575 [32:05<00:19,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 570/575 [32:08<00:15,  3.19s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 571/575 [32:11<00:12,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 572/575 [32:14<00:09,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|█████████▉| 573/575 [32:17<00:06,  3.18s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|█████████▉| 574/575 [32:20<00:03,  3.20s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 575/575 [32:24<00:00,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:***** Running Evaluation *****\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  Num examples = 483\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  Batch size = 8\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:***** Running Evaluation *****\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  Num examples = 483\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:  Batch size = 8\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/15 [00:00<?, ?it/s]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 2/15 [00:01<00:10,  1.26it/s]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 3/15 [00:03<00:13,  1.10s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 4/15 [00:04<00:14,  1.28s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])[1,mpirank:2,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 5/15 [00:06<00:13,  1.35s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 6/15 [00:07<00:12,  1.38s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) [1,mpirank:1,algo-1]<stdout>:Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 7/15 [00:09<00:11,  1.43s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval [1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])[1,mpirank:3,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 8/15 [00:10<00:09,  1.42s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 9/15 [00:11<00:08,  1.40s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 10/15 [00:13<00:06,  1.38s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 11/15 [00:14<00:05,  1.39s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: [1,mpirank:3,algo-1]<stdout>:torch.Size([8, 32]) Labels eval\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 12/15 [00:16<00:04,  1.42s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 13/15 [00:17<00:02,  1.44s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 14/15 [00:19<00:01,  1.45s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval [1,mpirank:0,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>:torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>:Logits shape eval: torch.Size([8, 32]) Labels eval torch.Size([8])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 15/15 [00:20<00:00,  1.40s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                                 #015\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                               #015#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'eval_accuracy': 0.041666666666666664, 'eval_precision': 0.05621891021804616, 'eval_recall': 0.041666666666666664, 'eval_f1': 0.03612288365195629, 'epoch': 5.0}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 575/575 [32:46<00:00,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 15/15 [00:20<00:00,  1.40s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                                 #015\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                               #015[1,mpirank:0,algo-1]<stderr>:#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'eval_accuracy': 0.041666666666666664, 'eval_precision': 0.05621891021804616, 'eval_recall': 0.041666666666666664, 'eval_f1': 0.03612288365195629, 'eval_epoch': 5.0, 'eval_runtime': 21.9763, 'eval_samples_per_second': 21.978, 'eval_steps_per_second': 0.728, 'epoch': 5.0}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 575/575 [32:46<00:00,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 15/15 [00:20<00:00,  1.40s/it]#033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015                                               #033[A\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>:  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:1,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:3,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:2,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Saving model checkpoint to /opt/ml/model/checkpoint-575\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Saving model checkpoint to /opt/ml/model/checkpoint-575\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Trainer.model is not a `PreTrainedModel`, only saving its state dict.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Trainer.model is not a `PreTrainedModel`, only saving its state dict.\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 13:14:51,490] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step575 is begin to save!\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 13:14:51,499] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /opt/ml/model/checkpoint-575/global_step575/zero_pp_rank_0_mp_rank_00_model_states.pt\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 13:14:53,591] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step575 is begin to save!\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:[2025-07-30 13:14:53,601] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /opt/ml/model/checkpoint-575/global_step575/zero_pp_rank_0_mp_rank_00_model_states.pt\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:{'train_runtime': 1972.2954, 'train_samples_per_second': 9.352, 'train_steps_per_second': 0.292, 'train_loss': 1.303603515625, 'epoch': 5.0}\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 575/575 [32:52<00:00,  3.17s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 575/575 [32:52<00:00,  3.43s/it]\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Configuration saved in /opt/ml/model/prefix_encoder/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Configuration saved in /opt/ml/model/prefix_encoder/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Model weights saved in /opt/ml/model/prefix_encoder/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Model weights saved in /opt/ml/model/prefix_encoder/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Configuration saved in /opt/ml/model/suffix_encoder/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Configuration saved in /opt/ml/model/suffix_encoder/config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Model weights saved in /opt/ml/model/suffix_encoder/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Model weights saved in /opt/ml/model/suffix_encoder/model.safetensors\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stderr>:Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001B[0m\n",
      "\u001B[34m[1,mpirank:0,algo-1]<stdout>:Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001B[0m\n",
      "\u001B[34m2025-07-30 13:15:01,158 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001B[0m\n",
      "\u001B[34m2025-07-30 13:15:01,158 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001B[0m\n",
      "\u001B[34m2025-07-30 13:15:01,158 sagemaker-training-toolkit INFO     Begin writing status file from leader node to worker nodes (if any)\u001B[0m\n",
      "\u001B[34m2025-07-30 13:15:31,158 sagemaker-training-toolkit INFO     Finished writing status file from leader node to worker nodes (if any)\u001B[0m\n",
      "\u001B[34m2025-07-30 13:15:31,159 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001B[0m\n",
      "\n",
      "2025-07-30 13:15:34 Uploading - Uploading generated training model\n",
      "2025-07-30 13:16:42 Completed - Training job completed\n",
      "Training seconds: 2403\n",
      "Billable seconds: 2403\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:30:56.274476Z",
     "start_time": "2025-07-30T12:30:55.381591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sagemaker.analytics import TrainingJobAnalytics\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name='huggingface-pytorch-training-2025-07-30-03-59-45-613').dataframe()\n",
    "print(df)"
   ],
   "id": "1b65b20af226c80b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.analytics:Warning: No metrics called eval_loss found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    timestamp              metric_name      value\n",
      "0         0.0                     loss   3.360500\n",
      "1       180.0                     loss   2.731600\n",
      "2         0.0            learning_rate   1.721739\n",
      "3       180.0            learning_rate   4.173913\n",
      "4         0.0            eval_accuracy   0.031250\n",
      "5         0.0                  eval_f1   0.019156\n",
      "6         0.0           eval_precision   0.022735\n",
      "7         0.0              eval_recall   0.031250\n",
      "8         0.0             eval_runtime  21.798900\n",
      "9         0.0  eval_samples_per_second  22.157000\n",
      "10        0.0                    epoch   0.430000\n",
      "11      180.0                    epoch   0.870000\n",
      "12      240.0                    epoch   1.000000\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ed2726b955e0932"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8d55b008d723c371"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legalpacaenv",
   "language": "python",
   "name": "lagalpacaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
